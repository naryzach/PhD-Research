{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM Multi-Target Fine-Tuning and Embedding Comparison\n",
    "\n",
    "This notebook walks through the process of using a pre-trained ESM model for a **multi-class classification** task. It generates protein embeddings, fine-tunes the model on multiple targets (including a 'Non-Binder' category), and compares the embedding space before and after fine-tuning using PCA and t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's install and import all the necessary libraries. Ensure you have a GPU available for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers datasets scikit-learn pandas torch seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import hashlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set all your parameters in this cell. This replaces the command-line arguments from the original script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # --- Input and Output ---\n",
    "    # IMPORTANT: Make sure this CSV file is uploaded to your notebook environment!\n",
    "    CSV_PATH = \"../Data/TIMP_binder_MMP9_AB.csv\"\n",
    "    OUTPUT_DIR = \"../Local/esm_multitarget_out\"\n",
    "    SEQ_COL = \"Full Seq\" # Column with protein sequences\n",
    "    LABEL_COL = \"Target\" # Column with target labels\n",
    "    BINDING_COL = \"Encoding\" # Column indicating positive (1) or negative (0) binding\n",
    "\n",
    "    # --- Model and Training ---\n",
    "    #MODEL_ID = \"facebook/esm2_t6_8M_UR50D\"\n",
    "    MODEL_ID = \"facebook/esm2_t30_150M_UR50D\"\n",
    "    #MODEL_ID = \"facebook/esm2_t33_650M_UR50D\"\n",
    "    #MODEL_ID = \"EvolutionaryScale/esmc-300m-2024-12\" # Use a newer ESM-C model. Not functional yet\n",
    "    TEST_SIZE = 0.2\n",
    "    EPOCHS = 25\n",
    "    LEARNING_RATE = 1e-3\n",
    "    DECAY_TYPE = 'linear'\n",
    "    BATCH_SIZE = 8\n",
    "    FORCE_RETRAIN = False # Set to True to retrain even if a saved model exists\n",
    "    TUNE_AGAIN = True\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "These are the utility functions from the script for device detection, embedding computation, and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Detects and returns the available hardware device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available(): # For Apple Silicon\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def compute_embeddings(sequences, model, tokenizer, device):\n",
    "    \"\"\"Computes embeddings for a list of sequences using the given model.\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for seq in tqdm(sequences, desc=\"Computing embeddings\"):\n",
    "            tokens = tokenizer(seq, return_tensors=\"pt\", truncation=True, max_length=1022).to(device)\n",
    "            output = model(**tokens, output_hidden_states=True)\n",
    "            # Use the last hidden state and average pool across the sequence length\n",
    "            embedding = output.hidden_states[-1].mean(dim=1).squeeze().cpu().numpy()\n",
    "            embeddings.append(embedding)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "sns.set_palette([\"#0000FF\",\"#FF0000\",\"#009100\",])\n",
    "\n",
    "def plot_single(data, labels, title, filename, hue_order=None):\n",
    "    \"\"\"Creates and saves a single scatter plot.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x=data[:, 0], y=data[:, 1], hue=labels, s=50, alpha=0.7, hue_order=hue_order)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.legend(title=\"Target\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show() # Display the plot inline\n",
    "    plt.close()\n",
    "\n",
    "def plot_comparison(before_data, after_data, labels, reduction_method, out_dir, model_tag, timestamp, hue_order=None):\n",
    "    \"\"\"Creates and saves a 2-panel comparison plot.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    fig.suptitle(f'{reduction_method} Comparison ({model_tag})', fontsize=20)\n",
    "\n",
    "    # Before fine-tuning\n",
    "    sns.scatterplot(ax=axes[0], x=before_data[:, 0], y=before_data[:, 1], hue=labels, s=50, alpha=0.7, hue_order=hue_order)\n",
    "    axes[0].set_title(\"Before Fine-Tuning\", fontsize=16)\n",
    "    axes[0].set_xlabel(\"Dimension 1\")\n",
    "    axes[0].set_ylabel(\"Dimension 2\")\n",
    "    axes[0].legend(title=\"Target\")\n",
    "\n",
    "    # After fine-tuning\n",
    "    sns.scatterplot(ax=axes[1], x=after_data[:, 0], y=after_data[:, 1], hue=labels, s=50, alpha=0.7, hue_order=hue_order)\n",
    "    axes[1].set_title(\"After Fine-Tuning\", fontsize=16)\n",
    "    axes[1].set_xlabel(\"Dimension 1\")\n",
    "    axes[1].set_ylabel(\"Dimension 2\")\n",
    "    axes[1].legend(title=\"Target\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    filename = out_dir / f\"{reduction_method.lower()}_comparison_{model_tag}_{timestamp}.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.show() # Display the plot inline\n",
    "    plt.close()\n",
    "\n",
    "class ProteinDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for protein sequences.\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def compute_metrics_multiclass(pred):\n",
    "    \"\"\"Computes detailed classification metrics for the multi-class Trainer.\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # Use 'weighted' average for multi-class classification to account for label imbalance\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Pipeline\n",
    "\n",
    "This is the main execution block. It will perform all the steps from the original script in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths and parameters\n",
    "out_dir = Path(config.OUTPUT_DIR)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "device = get_device()\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_tag = config.MODEL_ID.split('/')[-1]\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using model: {config.MODEL_ID}\")\n",
    "\n",
    "# Load and process data\n",
    "print(\"--- Loading and Processing Data ---\")\n",
    "df = pd.read_csv(config.CSV_PATH)\n",
    "df = df.dropna(subset=[config.SEQ_COL, config.LABEL_COL, config.BINDING_COL]) # Drop rows with missing critical info\n",
    "df = df.drop_duplicates(subset=[config.SEQ_COL]) # Remove duplicate sequences\n",
    "df = df.copy() # Avoid SettingWithCopyWarning\n",
    "\n",
    "# --- Preprocessing Step: Treat non-binders as a separate class ---\n",
    "df.loc[df[config.BINDING_COL] == 0, config.LABEL_COL] = 'Non-Binder'\n",
    "print(f\"Total sequences in dataset: {len(df)}\")\n",
    "\n",
    "# Convert text labels to integers\n",
    "unique_labels = sorted(df[config.LABEL_COL].unique()) # Sort for consistent mapping\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "df['numerical_labels'] = df[config.LABEL_COL].map(label2id)\n",
    "\n",
    "num_labels = len(unique_labels)\n",
    "print(f\"Found {num_labels} unique classes for training: {', '.join(unique_labels)}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df[config.LABEL_COL].value_counts())\n",
    "\n",
    "sequences = df[config.SEQ_COL].tolist()\n",
    "labels = df['numerical_labels'].tolist()\n",
    "text_labels = df[config.LABEL_COL].tolist() # For plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Pre-trained Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 1: Pre-trained Model Embeddings ---\")\n",
    "\n",
    "# Caching setup for embeddings\n",
    "sequences_hash = hashlib.md5(\"\".join(sequences).encode()).hexdigest()\n",
    "emb_cache_file = out_dir / f\"embeddings_before_{model_tag}_{sequences_hash}.npy\"\n",
    "\n",
    "if emb_cache_file.exists():\n",
    "    print(f\"Loading pre-trained embeddings from cache: {emb_cache_file}\")\n",
    "    embeddings_before = np.load(emb_cache_file)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_ID)\n",
    "    trust_code = 'esm' in config.MODEL_ID.lower()\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config.MODEL_ID, \n",
    "        num_labels=num_labels, \n",
    "        trust_remote_code=trust_code\n",
    "    )\n",
    "    embeddings_before = compute_embeddings(sequences, model, tokenizer, device)\n",
    "    np.save(emb_cache_file, embeddings_before)\n",
    "    print(f\"Saved pre-trained embeddings to cache: {emb_cache_file}\")\n",
    "\n",
    "# PCA and t-SNE on pre-trained embeddings\n",
    "print(\"\\nRunning PCA and t-SNE on pre-trained embeddings...\")\n",
    "pca_before = PCA(n_components=2).fit_transform(embeddings_before)\n",
    "tsne_before = TSNE(n_components=2, perplexity=min(30, len(df)-1), random_state=42).fit_transform(embeddings_before)\n",
    "\n",
    "# Save results\n",
    "pca_before_csv = out_dir / f'pca_before_{model_tag}_{timestamp}.csv'\n",
    "tsne_before_csv = out_dir / f'tsne_before_{model_tag}_{timestamp}.csv'\n",
    "pd.DataFrame(pca_before, columns=['PC1', 'PC2']).to_csv(pca_before_csv, index=False)\n",
    "pd.DataFrame(tsne_before, columns=['TSNE1', 'TSNE2']).to_csv(tsne_before_csv, index=False)\n",
    "print(\"Saved PCA and t-SNE results for pre-trained embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fine-tuning Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 2: Fine-tuning Classification Head ---\")\n",
    "\n",
    "finetuned_model_path = out_dir / f\"finetuned_{model_tag}\"\n",
    "\n",
    "# We need the validation set later for the classification report\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(sequences, labels, test_size=config.TEST_SIZE, random_state=42, stratify=labels)\n",
    "\n",
    "if not finetuned_model_path.exists() or config.FORCE_RETRAIN or config.TUNE_AGAIN:\n",
    "    if config.FORCE_RETRAIN and finetuned_model_path.exists():\n",
    "        print(\"Forcing re-training.\")\n",
    "\n",
    "    if config.TUNE_AGAIN:\n",
    "        print(\"Using stored weights and re-training\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(finetuned_model_path)\n",
    "    else:\n",
    "        print(\"Training model frome the source\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.MODEL_ID)\n",
    "        trust_code = 'esm' in config.MODEL_ID.lower()\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config.MODEL_ID, \n",
    "            num_labels=num_labels,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            trust_remote_code=trust_code\n",
    "        )\n",
    "\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "    val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "    train_dataset = ProteinDataset(train_encodings, train_labels)\n",
    "    val_dataset = ProteinDataset(val_encodings, val_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(out_dir / 'training_checkpoints'),\n",
    "        num_train_epochs=config.EPOCHS,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        #learning_rate=config.LEARNING_RATE,\n",
    "        #lr_scheduler_type=config.DECAY_TYPE,\n",
    "        logging_dir=str(out_dir / 'logs'),\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\", # Using f1 for best model selection\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics_multiclass, # Use the multi-class metrics function\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(finetuned_model_path)\n",
    "    tokenizer.save_pretrained(finetuned_model_path)\n",
    "    print(f\"Fine-tuned model saved to {finetuned_model_path}\")\n",
    "\n",
    "    print(\"\\n--- Generating Validation Curve ---\")\n",
    "    # Extract log history\n",
    "    log_history = trainer.state.log_history\n",
    "    \n",
    "    # Use pandas to easily separate training and validation logs\n",
    "    df = pd.DataFrame(log_history)\n",
    "    \n",
    "    # Get training and validation loss data\n",
    "    train_logs = df[df['loss'].notna()].dropna(axis=1, how='all').reset_index(drop=True)\n",
    "    eval_logs = df[df['eval_loss'].notna()].dropna(axis=1, how='all').reset_index(drop=True)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_logs['step'], train_logs['loss'], label='Training Loss')\n",
    "    plt.plot(eval_logs['step'], eval_logs['eval_loss'], label='Validation Loss', marker='o')\n",
    "    \n",
    "    plt.title('Training and Validation Loss Curve')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(out_dir / f\"validation_curve_{model_tag}_{timestamp}.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "else:\n",
    "    print(f\"Found existing fine-tuned model. Loading from {finetuned_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b: Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 2b: Detailed Classification Report ---\")\n",
    "\n",
    "# Load the best model\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(finetuned_model_path)\n",
    "model.to(device)\n",
    "\n",
    "# Create a dataset for the validation texts\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "val_dataset = ProteinDataset(val_encodings, val_labels)\n",
    "\n",
    "# Re-initialize Trainer to use its `predict` method\n",
    "trainer = Trainer(model=model)\n",
    "predictions, _, _ = trainer.predict(val_dataset)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(f'Classification Report for model: {model_tag}\\n')\n",
    "report_text = classification_report(val_labels, y_pred, target_names=list(label2id.keys()), zero_division=0)\n",
    "print(report_text)\n",
    "\n",
    "# Save report to a file\n",
    "class_report_path = out_dir / f'classification_report_{model_tag}_{timestamp}.txt'\n",
    "with open(class_report_path, 'w') as f:\n",
    "    f.write(report_text)\n",
    "print(f\"\\nClassification report saved to {class_report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Post-fine-tuning Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 3: Post-fine-tuning Model Embeddings ---\")\n",
    "\n",
    "emb_after_cache_file = out_dir / f\"embeddings_after_{model_tag}_{sequences_hash}.npy\"\n",
    "\n",
    "if emb_after_cache_file.exists() and not config.FORCE_RETRAIN:\n",
    "    print(f\"Loading fine-tuned embeddings from cache: {emb_after_cache_file}\")\n",
    "    embeddings_after = np.load(emb_after_cache_file)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)\n",
    "    trust_code = 'esm' in config.MODEL_ID.lower()\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        finetuned_model_path, \n",
    "        trust_remote_code=trust_code\n",
    "    )\n",
    "    embeddings_after = compute_embeddings(sequences, model, tokenizer, device)\n",
    "    np.save(emb_after_cache_file, embeddings_after)\n",
    "    print(f\"Saved fine-tuned embeddings to cache: {emb_after_cache_file}\")\n",
    "\n",
    "# PCA and t-SNE on fine-tuned embeddings\n",
    "print(\"\\nRunning PCA and t-SNE on fine-tuned embeddings...\")\n",
    "pca_after = PCA(n_components=2).fit_transform(embeddings_after)\n",
    "tsne_after = TSNE(n_components=2, perplexity=min(30, len(df)-1), random_state=42).fit_transform(embeddings_after)\n",
    "\n",
    "# Save results\n",
    "pca_after_csv = out_dir / f'pca_after_{model_tag}_{timestamp}.csv'\n",
    "tsne_after_csv = out_dir / f'tsne_after_{model_tag}_{timestamp}.csv'\n",
    "pd.DataFrame(pca_after, columns=['PC1', 'PC2']).to_csv(pca_after_csv, index=False)\n",
    "pd.DataFrame(tsne_after, columns=['TSNE1', 'TSNE2']).to_csv(tsne_after_csv, index=False)\n",
    "print(\"Saved PCA and t-SNE results for fine-tuned embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Generating Plots and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 4: Generating Plots ---\")\n",
    "hue_order = list(label2id.keys())\n",
    "\n",
    "# Define filenames for summary\n",
    "pca_before_png = out_dir / f\"pca_before_{model_tag}_{timestamp}.png\"\n",
    "pca_after_png = out_dir / f\"pca_after_{model_tag}_{timestamp}.png\"\n",
    "tsne_before_png = out_dir / f\"tsne_before_{model_tag}_{timestamp}.png\"\n",
    "tsne_after_png = out_dir / f\"tsne_after_{model_tag}_{timestamp}.png\"\n",
    "pca_comp_png = out_dir / f\"pca_comparison_{model_tag}_{timestamp}.png\"\n",
    "tsne_comp_png = out_dir / f\"tsne_comparison_{model_tag}_{timestamp}.png\"\n",
    "\n",
    "# Individual plots\n",
    "print(\"\\nPCA before fine-tuning:\")\n",
    "plot_single(pca_before, text_labels, \"PCA - Before Fine-tuning\", pca_before_png, hue_order=hue_order)\n",
    "print(\"\\nPCA after fine-tuning:\")\n",
    "plot_single(pca_after, text_labels, \"PCA - After Fine-tuning\", pca_after_png, hue_order=hue_order)\n",
    "print(\"\\nt-SNE before fine-tuning:\")\n",
    "plot_single(tsne_before, text_labels, \"t-SNE - Before Fine-tuning\", tsne_before_png, hue_order=hue_order)\n",
    "print(\"\\nt-SNE after fine-tuning:\")\n",
    "plot_single(tsne_after, text_labels, \"t-SNE - After Fine-tuning\", tsne_after_png, hue_order=hue_order)\n",
    "\n",
    "# Comparison plots\n",
    "print(\"\\nPCA Comparison:\")\n",
    "plot_comparison(pca_before, pca_after, text_labels, \"PCA\", out_dir, model_tag, timestamp, hue_order=hue_order)\n",
    "print(\"\\nt-SNE Comparison:\")\n",
    "plot_comparison(tsne_before, tsne_after, text_labels, \"t-SNE\", out_dir, model_tag, timestamp, hue_order=hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[DONE] All tasks completed!\")\n",
    "print(\"\\n--- Summary of Output Files ---\")\n",
    "print(f\"- Fine-tuned model saved to: {finetuned_model_path}\")\n",
    "print(f\"- Classification report saved to: {class_report_path}\")\n",
    "print(\"\\n- Embeddings:\")\n",
    "print(f\"  - Before fine-tuning (cached): {emb_cache_file}\")\n",
    "print(f\"  - After fine-tuning (cached): {emb_after_cache_file}\")\n",
    "print(\"\\n- CSV Coordinates:\")\n",
    "print(f\"  - PCA (before): {pca_before_csv}\")\n",
    "print(f\"  - t-SNE (before): {tsne_before_csv}\")\n",
    "print(f\"  - PCA (after): {pca_after_csv}\")\n",
    "print(f\"  - t-SNE (after): {tsne_after_csv}\")\n",
    "print(\"\\n- Plots:\")\n",
    "print(f\"  - PCA Comparison: {pca_comp_png}\")\n",
    "print(f\"  - t-SNE Comparison: {tsne_comp_png}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
