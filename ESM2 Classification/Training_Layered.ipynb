{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM Multi-Target Hierarchical Fine-Tuning and Embedding Comparison\n",
    "\n",
    "This notebook walks through a layered process of fine-tuning a pre-trained ESM model. It first trains on an entire dataset, then iteratively fine-tunes new models on specific subsets of the data in a hierarchical fashion.\n",
    "\n",
    "For each level of fine-tuning, it:\n",
    "1.  Generates protein embeddings from the current base model.\n",
    "2.  Fine-tunes a new model on a specific subset of data.\n",
    "3.  Generates embeddings from the newly fine-tuned model.\n",
    "4.  Produces a classification report, training validation curve, and comparison plots (PCA and t-SNE) for that specific level.\n",
    "5.  Saves all artifacts into a dedicated, nested directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's install and import all the necessary libraries. Ensure you have a GPU available for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers datasets scikit-learn numpy pandas torch seaborn matplotlib accelerate\n",
    "%pip install -q esm biopython py3Dmol huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.special import softmax\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score, fbeta_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from functools import partial\n",
    "import hashlib\n",
    "import json\n",
    "import shutil\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set all your parameters in this cell. This replaces the command-line arguments from the original script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # --- Input and Output ---\\n\",\n",
    "    # IMPORTANT: Make sure this CSV file is uploaded to your notebook environment!\n",
    "    CSV_PATH = \"../Data/TIMP_binder_all.csv\"\n",
    "    OUTPUT_DIR = \"../Local/esm_hierarchical_out\"\n",
    "    SEQ_COL = \"Full Seq\"       # Column with protein sequences\n",
    "    LABEL_COL = \"Target\" # The ultimate target column for classification\n",
    "    BINDING_COL = \"Encoding\"   # Column indicating positive (1) or negative (0) binding\n",
    "    COUNT_COL = \"Count\"\n",
    "\n",
    "    # --- Hierarchy Definition ---\n",
    "    # NOTE: HIERARCHY_COLS should not contain LABEL_COL\n",
    "    HIERARCHY_COLS = [\"Ligand_Subtype\", \"Loop_Subtype\"] # <-- IMPORTANT: UPDATE THIS LIST\n",
    "\n",
    "    # --- Model and Training ---\\n\",\n",
    "    #MODEL_ID = \"facebook/esm2_t36_3B_UR50D\"\n",
    "    #MODEL_ID = \"facebook/esm2_t33_650M_UR50D\"\n",
    "    MODEL_ID = \"facebook/esm2_t30_150M_UR50D\"\n",
    "    #MODEL_ID = \"facebook/esm2_t6_8M_UR50D\" # Smaller model for testing\n",
    "    TEST_SIZE = 0.2\n",
    "    EPOCHS = 25\n",
    "    LEARNING_RATE = 1e-3\n",
    "    BATCH_SIZE = 8\n",
    "    FORCE_RETRAIN = False # Set to True to retrain even if a saved model exists\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "These are the utility functions from the script for device detection, embedding computation, and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Detects and returns the available hardware device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available(): # For Apple Silicon\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def compute_embeddings(sequences, model, tokenizer, device, run_name=\"\"):\n",
    "    \"\"\"Computes embeddings for a list of sequences using the given model.\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for seq in tqdm(sequences, desc=f\"Computing embeddings ({run_name})\"):\n",
    "            tokens = tokenizer(seq, return_tensors=\"pt\", truncation=True, max_length=1022).to(device)\n",
    "            output = model(**tokens, output_hidden_states=True)\n",
    "            # Use the last hidden state and average pool across the sequence length\n",
    "            embedding = output.hidden_states[-1].mean(dim=1).squeeze().cpu().numpy()\n",
    "            embeddings.append(embedding)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "sns.set_palette([\"#009100\",\"#FF0000\",\"#0000FF\",\"#800080\",\"#FFA500\",\"#00FFFF\",\"#FFC0CB\",\"#A52A2A\",\"#808000\",\"#000000\"])\n",
    "\n",
    "def plot_comparison(before_data, after_data, labels, reduction_method, out_dir, model_tag, timestamp, hue_order=None, run_name=\"\"):\n",
    "    \"\"\"Creates and saves a 2-panel comparison plot.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    title = f'{reduction_method} Comparison for {run_name} ({model_tag})'\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "\n",
    "    # Before fine-tuning\n",
    "    sns.scatterplot(ax=axes[0], x=before_data[:, 0], y=before_data[:, 1], hue=labels, s=50, alpha=0.7, hue_order=hue_order)\n",
    "    axes[0].set_title(\"Before This Training Step\", fontsize=16)\n",
    "    axes[0].set_xlabel(\"Dimension 1\")\n",
    "    axes[0].set_ylabel(\"Dimension 2\")\n",
    "    axes[0].legend(title=\"Target\")\n",
    "\n",
    "    # After fine-tuning\n",
    "    sns.scatterplot(ax=axes[1], x=after_data[:, 0], y=after_data[:, 1], hue=labels, s=50, alpha=0.7, hue_order=hue_order)\n",
    "    axes[1].set_title(\"After This Training Step\", fontsize=16)\n",
    "    axes[1].set_xlabel(\"Dimension 1\")\n",
    "    axes[1].set_ylabel(\"Dimension 2\")\n",
    "    axes[1].legend(title=\"Target\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    filename = out_dir / f\"{reduction_method.lower()}_comparison_{model_tag}_{timestamp}.png\"\n",
    "    plt.savefig(filename)\n",
    "    print(f\"Saved comparison plot to: {filename}\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "class ProteinDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for protein sequences.\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "def compute_metrics_multiclass(pred, id2label: dict):\n",
    "    \"\"\"\n",
    "    Computes generalized classification metrics with a special focus on binder-only performance.\n",
    "    \n",
    "    This function dynamically identifies the 'Non-Binder' class and calculates metrics\n",
    "    only for the true binder classes.\n",
    "    \"\"\"\n",
    "    BETA = 0.5\n",
    "\n",
    "    labels = pred.label_ids\n",
    "    preds_data = pred.predictions[0] if isinstance(pred.predictions, tuple) else pred.predictions\n",
    "    preds = np.argmax(preds_data, axis=-1)\n",
    "\n",
    "    # --- Find the 'Non-Binder' label ID dynamically ---\n",
    "    # We iterate through the id2label mapping to find the key (the integer label)\n",
    "    # corresponding to the value 'Non-Binder'.\n",
    "    non_binder_id = None\n",
    "    for label_id, label_name in id2label.items():\n",
    "        # Using .lower() makes the check case-insensitive (e.g., 'non-binder', 'Non-Binder')\n",
    "        if \"non-binder\" in label_name.lower():\n",
    "            non_binder_id = int(label_id)\n",
    "            break\n",
    "            \n",
    "    # --- Create the list of binder labels by excluding the non-binder ---\n",
    "    all_label_ids = list(id2label.keys())\n",
    "    \n",
    "    # Check if a non-binder class was found before trying to remove it\n",
    "    if non_binder_id is not None:\n",
    "        # The list of binder_labels is all labels EXCEPT the non_binder_id\n",
    "        binder_labels = [label for label in all_label_ids if label != non_binder_id]\n",
    "    else:\n",
    "        # If no non-binder class is found, assume all classes are binders\n",
    "        binder_labels = all_label_ids\n",
    "        print(\"Warning: 'Non-Binder' class not found in id2label dict. Calculating metrics on all classes.\")\n",
    "\n",
    "\n",
    "    # --- Calculate your binder-focused metrics as before ---\n",
    "    # F-beta score with beta=0.5 to heavily favor precision on binder classes\n",
    "    binder_fbeta_score = fbeta_score(\n",
    "        labels, \n",
    "        preds, \n",
    "        beta=BETA, \n",
    "        labels=binder_labels, # Using the dynamically generated list\n",
    "        average='macro', \n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    # Precision for only binder classes\n",
    "    binder_precision, binder_recall, binder_f1, _ = precision_recall_fscore_support(\n",
    "        labels, \n",
    "        preds, \n",
    "        labels=binder_labels, # Using the dynamically generated list\n",
    "        average='macro', \n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    # Old Metrics\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "    f_beta_macro = fbeta_score(labels, preds, beta=BETA, average='macro', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f_beta_macro': f_beta_macro,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'recall_macro': recall_macro,\n",
    "        'binder_precision': binder_precision,\n",
    "        'binder_recall': binder_recall,\n",
    "        'binder_f1': binder_f1,\n",
    "        'binder_fbeta_score': binder_fbeta_score # For training\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Updated run_finetuning_pipeline Function\n",
    "\n",
    "def run_finetuning_pipeline(train_df: pd.DataFrame, validation_df: pd.DataFrame, output_path: Path, base_model_id: str, run_name: str, config: Config):\n",
    "    \"\"\"\n",
    "    Runs the complete fine-tuning and analysis pipeline using pre-split train and validation dataframes.\n",
    "    \"\"\"\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    device = get_device()\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_tag = config.MODEL_ID.split('/')[-1]\n",
    "    finetuned_model_path = output_path / f\"finetuned_{model_tag}\"\n",
    "\n",
    "    # For plotting and embedding, we use the combined data from this run\n",
    "    df_subset = pd.concat([train_df, validation_df])\n",
    "\n",
    "    print(f\"--- Starting Pipeline for: {run_name} ---\")\n",
    "    print(f\"Training data size: {len(train_df)}, Validation data size: {len(validation_df)}\")\n",
    "    print(f\"Output directory: {output_path}\")\n",
    "    print(f\"Base model: {base_model_id}\")\n",
    "\n",
    "    # --- 1. Data Processing for the current subset ---\n",
    "    unique_labels = sorted(df_subset[config.LABEL_COL].unique())\n",
    "    label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "    \n",
    "    # Apply mapping to both dataframes\n",
    "    train_df['numerical_labels'] = train_df[config.LABEL_COL].map(label2id)\n",
    "    validation_df['numerical_labels'] = validation_df[config.LABEL_COL].map(label2id)\n",
    "    num_labels = len(unique_labels)\n",
    "    \n",
    "    # The rest of the script uses these separate lists\n",
    "    train_texts, train_labels = train_df[config.SEQ_COL].tolist(), train_df['numerical_labels'].tolist()\n",
    "    val_texts, val_labels = validation_df[config.SEQ_COL].tolist(), validation_df['numerical_labels'].tolist()\n",
    "\n",
    "    # Data for plots\n",
    "    sequences = df_subset[config.SEQ_COL].tolist()\n",
    "    text_labels = df_subset[config.LABEL_COL].tolist()\n",
    "\n",
    "    # --- 2. Pre-trained Model Embeddings ---\n",
    "    print(\"\\nStep A: Generating embeddings from the BASE model...\")\n",
    "    trust_code = 'esm' in base_model_id.lower() or 'synthyra' in base_model_id.lower()\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_id, num_labels=num_labels, trust_remote_code=trust_code, ignore_mismatched_sizes=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_ID if 'esm2' in config.MODEL_ID else base_model_id)\n",
    "    embeddings_before = compute_embeddings(sequences, base_model, tokenizer, device, run_name)\n",
    "\n",
    "    # --- 3. Fine-tuning the Model ---\n",
    "    print(\"\\nStep B: Fine-tuning the model...\")\n",
    "    has_validation_data = not validation_df.empty\n",
    "    \n",
    "    if finetuned_model_path.exists() and not config.FORCE_RETRAIN:\n",
    "        print(f\"Found existing fine-tuned model. Loading from {finetuned_model_path}\")\n",
    "    else:\n",
    "        # (Code for re-training is largely the same, but uses pre-split data)\n",
    "        model_for_training = AutoModelForSequenceClassification.from_pretrained(\n",
    "            base_model_id, num_labels=num_labels, id2label=id2label, label2id=label2id, trust_remote_code=trust_code, ignore_mismatched_sizes=True\n",
    "        )\n",
    "\n",
    "        # The id2label dictionary is created from your LabelEncoder or is in your model config\n",
    "        id2label_map = model_for_training.config.id2label\n",
    "        compute_metrics_with_context = partial(compute_metrics_multiclass, id2label=id2label_map) # Create a partial function with the id2label map baked in\n",
    "        \n",
    "        train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "        train_dataset = ProteinDataset(train_encodings, train_labels)\n",
    "        \n",
    "        # Only create validation dataset if validation_df is not empty\n",
    "        val_dataset = None\n",
    "        if has_validation_data:\n",
    "            val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "            val_dataset = ProteinDataset(val_encodings, val_labels)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(output_path / 'training_checkpoints'),\n",
    "            num_train_epochs=config.EPOCHS,\n",
    "            per_device_train_batch_size=config.BATCH_SIZE,\n",
    "            per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "            warmup_steps=100,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=str(output_path / 'logs'),\n",
    "            logging_steps=10,\n",
    "            # Adjust strategy based on presence of validation data\n",
    "            evaluation_strategy=\"epoch\" if has_validation_data else \"no\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=has_validation_data,\n",
    "            metric_for_best_model=\"binder_fbeta_score\" if has_validation_data else None,\n",
    "            greater_is_better=True\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model_for_training, \n",
    "            args=training_args, \n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset, \n",
    "            #compute_metrics=compute_metrics_multiclass,\n",
    "            compute_metrics=compute_metrics_with_context,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] if has_validation_data else None\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        trainer.save_model(finetuned_model_path)\n",
    "        tokenizer.save_pretrained(finetuned_model_path)\n",
    "        print(f\"Fine-tuned model saved to {finetuned_model_path}\")\n",
    "        \n",
    "        if has_validation_data:\n",
    "            # Plot validation curve\n",
    "            df_history = pd.DataFrame(trainer.state.log_history)\n",
    "            train_loss = df_history[df_history['loss'].notna()]\n",
    "            eval_loss = df_history[df_history['eval_loss'].notna()]\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(train_loss['step'], train_loss['loss'], label='Training Loss')\n",
    "            plt.plot(eval_loss['step'], eval_loss['eval_loss'], label='Validation Loss', marker='o')\n",
    "            plt.title(f'Training and Validation Loss ({run_name})')\n",
    "            plt.xlabel('Training Steps'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
    "            val_curve_path = output_path / f\"validation_curve_{model_tag}_{timestamp}.png\"\n",
    "            plt.savefig(val_curve_path)\n",
    "            plt.show(); plt.close()\n",
    "            print(f\"Validation curve saved to {val_curve_path}\")\n",
    "\n",
    "    # --- 4. Detailed Classification Report on Validation Set ---\n",
    "    if has_validation_data:\n",
    "        print(\"\\nStep C: Generating classification report on the fixed validation set...\")\n",
    "        # ... (The rest of the function proceeds as before, using the validation data)\n",
    "        # The logic for reporting, embedding, and plotting remains the same.\n",
    "        finetuned_model = AutoModelForSequenceClassification.from_pretrained(finetuned_model_path)\n",
    "        report_trainer = Trainer(model=finetuned_model.to(device))\n",
    "        predictions, _, _ = report_trainer.predict(val_dataset)\n",
    "        y_pred = np.argmax(predictions, axis=1)\n",
    "        report_text = classification_report(val_labels, y_pred, target_names=list(label2id.keys()), zero_division=0)\n",
    "        print(f'Classification Report for {run_name}:\\n{report_text}')\n",
    "        class_report_path = output_path / f'classification_report_{model_tag}_{timestamp}.txt'\n",
    "        with open(class_report_path, 'w') as f:\n",
    "            f.write(f\"Classification Report for run: {run_name}\\n\\n{report_text}\")\n",
    "        print(f\"Classification report saved to {class_report_path}\")\n",
    "\n",
    "        # --- Expanded classification report ---\n",
    "\n",
    "        # Create the partial function to pass the id2label map\n",
    "        compute_metrics_with_context = partial(compute_metrics_multiclass, id2label=id2label_map)\n",
    "\n",
    "        # Get predictions for the hold-out test set\n",
    "        print(\"--- Generating Predictions on the Hold-Out Test Set ---\")\n",
    "        final_report_text = report_text\n",
    "\n",
    "        # Calculate ALL custom metrics using your function\n",
    "        predictions = trainer.predict(val_dataset)\n",
    "        all_custom_metrics = compute_metrics_with_context(predictions)\n",
    "\n",
    "        # 6. Dynamically create the custom metrics report text\n",
    "        custom_metrics_header = \"--- Comprehensive Performance Metrics ---\"\n",
    "        custom_metrics_lines = []\n",
    "        for metric_name, metric_value in all_custom_metrics.items():\n",
    "            # Nicely format the name (e.g., 'binder_fbeta_score' -> 'Binder Fbeta Score')\n",
    "            formatted_name = metric_name.replace('_', ' ').replace('fbeta', 'F-beta').title()\n",
    "            custom_metrics_lines.append(f\"{formatted_name}: {metric_value:.4f}\")\n",
    "\n",
    "        custom_metrics_text = \"\\n\".join([custom_metrics_header] + custom_metrics_lines)\n",
    "\n",
    "        # 7. Print and save the combined, comprehensive report\n",
    "        print(final_report_text)\n",
    "        print(custom_metrics_text)\n",
    "\n",
    "        final_report_path = Path(config.OUTPUT_DIR) / f'Expanded_classification_report_{run_name}.txt'\n",
    "\n",
    "        with open(final_report_path, 'w') as f:\n",
    "            f.write(f\"--- Final Model Evaluation on the Hold-Out Test Set ---\\n\")\n",
    "            f.write(f\"Model: {run_name}\\n\\n\")\n",
    "            \n",
    "            # Write the standard Sklearn report\n",
    "            f.write(\"--- Standard Classification Report ---\\n\")\n",
    "            f.write(final_report_text)\n",
    "            f.write(\"\\n\\n\") # Add spacing\n",
    "            \n",
    "            # Write the dynamically generated custom metrics report\n",
    "            f.write(custom_metrics_text)\n",
    "\n",
    "        print(f\"\\\\nFinal, comprehensive report saved to: {final_report_path}\")\n",
    "\n",
    "\n",
    "    # --- 5. Post-fine-tuning Embeddings & Plotting ---\n",
    "    print(\"\\nStep D: Generating embeddings from the NEWLY fine-tuned model...\")\n",
    "    finetuned_model_loaded = AutoModelForSequenceClassification.from_pretrained(finetuned_model_path)\n",
    "    embeddings_after = compute_embeddings(sequences, finetuned_model_loaded, tokenizer, device, run_name)\n",
    "    print(\"\\nStep E: Running PCA/t-SNE and generating comparison plots...\")\n",
    "    perplexity = min(30, len(df_subset) - 1) if len(df_subset) > 1 else 1\n",
    "    pca_before = PCA(n_components=2).fit_transform(embeddings_before)\n",
    "    tsne_before = TSNE(n_components=2, perplexity=perplexity, random_state=42, max_iter=1000).fit_transform(embeddings_before)\n",
    "    pca_after = PCA(n_components=2).fit_transform(embeddings_after)\n",
    "    tsne_after = TSNE(n_components=2, perplexity=perplexity, random_state=42, max_iter=1000).fit_transform(embeddings_after)\n",
    "    hue_order = list(label2id.keys())\n",
    "    plot_comparison(pca_before, pca_after, text_labels, \"PCA\", output_path, model_tag, timestamp, hue_order=hue_order, run_name=run_name)\n",
    "    plot_comparison(tsne_before, tsne_after, text_labels, \"t-SNE\", output_path, model_tag, timestamp, hue_order=hue_order, run_name=run_name)\n",
    "    \n",
    "    print(f\"--- Finished Pipeline for: {run_name} ---\\n\")\n",
    "    return finetuned_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Updated Hierarchical Execution Logic\n",
    "\n",
    "def process_level(df_train: pd.DataFrame, df_validation: pd.DataFrame, parent_model_path: Path, hierarchy_level: int, config: Config, parent_output_path: Path):\n",
    "    \"\"\"\n",
    "    Recursively processes each level of the hierarchy using pre-split data.\n",
    "    \"\"\"\n",
    "    if hierarchy_level >= len(config.HIERARCHY_COLS):\n",
    "        return\n",
    "\n",
    "    col_name = config.HIERARCHY_COLS[hierarchy_level]\n",
    "    print(f\"==================== STARTING HIERARCHY LEVEL {hierarchy_level + 1} (Column: '{col_name}') ====================\")\n",
    "    \n",
    "    # Iterate through categories present in the TRAINING data for this level\n",
    "    for category in sorted(df_train[col_name].unique()):\n",
    "        print(f\"\\nProcessing Category: '{category}' from column '{col_name}'\")\n",
    "        \n",
    "        # Filter both the training and validation sets for the current category\n",
    "        train_subset = df_train[df_train[col_name] == category].copy()\n",
    "        validation_subset = df_validation[df_validation[col_name] == category].copy()\n",
    "\n",
    "        # Check for sufficient data\n",
    "        if len(train_subset) < config.BATCH_SIZE or len(train_subset[config.LABEL_COL].unique()) < 2:\n",
    "            print(f\"SKIPPING '{category}': Insufficient training data or fewer than 2 classes.\")\n",
    "            continue\n",
    "        if validation_subset.empty:\n",
    "            print(f\"WARNING: No validation data found for '{category}'. Training will proceed without evaluation-based early stopping.\")\n",
    "\n",
    "        safe_category_name = \"\".join(c for c in category if c.isalnum() or c in (' ', '_')).rstrip()\n",
    "        run_name = f\"L{hierarchy_level+1}_{safe_category_name.replace(' ', '_')}\"\n",
    "        output_path = parent_output_path / run_name\n",
    "        \n",
    "        newly_tuned_model_path = run_finetuning_pipeline(\n",
    "            train_df=train_subset,\n",
    "            validation_df=validation_subset, # Pass the filtered validation set\n",
    "            output_path=output_path,\n",
    "            base_model_id=str(parent_model_path),\n",
    "            run_name=run_name,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        # Recursive call for the next level\n",
    "        process_level(\n",
    "            df_train=train_subset, # Pass filtered subsets to the next level\n",
    "            df_validation=validation_subset,\n",
    "            parent_model_path=newly_tuned_model_path,\n",
    "            hierarchy_level=hierarchy_level + 1,\n",
    "            config=config,\n",
    "            parent_output_path=output_path\n",
    "        )\n",
    "    print(f\"==================== FINISHED HIERARCHY LEVEL {hierarchy_level + 1} ====================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Pipeline\n",
    "\n",
    "This is the main execution block. It will perform all the steps from the original script in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Loading and Preparing Initial Data ---\")\n",
    "# (Data loading and cleaning is the same)\n",
    "main_df = pd.read_csv(config.CSV_PATH)\n",
    "main_df = main_df.dropna(subset=[config.SEQ_COL, config.LABEL_COL, config.BINDING_COL])\n",
    "main_df = main_df.drop_duplicates(subset=[config.SEQ_COL])\n",
    "main_df = main_df.copy()\n",
    "main_df.loc[main_df[config.BINDING_COL] == 0, config.LABEL_COL] = 'Non-Binder'\n",
    "\n",
    "# --- NEW: THREE-WAY DATA SPLIT ---\n",
    "print(\"\\n--- Creating fixed Train, Validation, and Test sets ---\")\n",
    "# 1. Split off the final, untouchable test set\n",
    "main_train_val_df, final_test_df = train_test_split(\n",
    "    main_df, test_size=0.2, random_state=42, stratify=main_df[config.LABEL_COL]\n",
    ")\n",
    "# 2. Split the remaining data into a training set and a fixed validation set\n",
    "main_train_df, global_validation_df = train_test_split(\n",
    "    main_train_val_df, test_size=0.15, random_state=42, stratify=main_train_val_df[config.LABEL_COL]\n",
    ")\n",
    "print(f\"Total training examples: {len(main_train_df)}\")\n",
    "print(f\"Total (fixed) validation examples: {len(global_validation_df)}\")\n",
    "print(f\"Total (hold-out) test examples: {len(final_test_df)}\")\n",
    "\n",
    "# --- LEVEL 0: GLOBAL FINE-TUNING ---\n",
    "print(\"\\n==================== STARTING LEVEL 0 (GLOBAL FINE-TUNING) ====================\")\n",
    "global_output_path = Path(config.OUTPUT_DIR) / \"L0_Global_Finetuning\"\n",
    "global_model_path = run_finetuning_pipeline(\n",
    "    train_df=main_train_df, # Pass the dedicated training set\n",
    "    validation_df=global_validation_df, # Pass the dedicated validation set\n",
    "    output_path=global_output_path,\n",
    "    base_model_id=config.MODEL_ID,\n",
    "    run_name=\"L0_Global\",\n",
    "    config=config\n",
    ")\n",
    "print(\"==================== FINISHED LEVEL 0 ====================\")\n",
    "\n",
    "# --- START HIERARCHICAL FINE-TUNING (LEVEL 1 and beyond) ---\n",
    "if config.HIERARCHY_COLS:\n",
    "    process_level(\n",
    "        df_train=main_train_df, # Start recursion with the main train/validation sets\n",
    "        df_validation=global_validation_df,\n",
    "        parent_model_path=global_model_path,\n",
    "        hierarchy_level=0,\n",
    "        config=config,\n",
    "        parent_output_path=global_output_path\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nNo hierarchy columns defined. Skipping layered fine-tuning.\")\n",
    "\n",
    "print(\"\\n[DONE] All hierarchical fine-tuning tasks completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Final Unbiased Evaluation on the Hold-Out Test Set for ALL Models\n",
    "\n",
    "print(\"\\n==================== FINAL EVALUATION ON HOLD-OUT TEST SET ====================\")\n",
    "\n",
    "# --- 1. Find all trained models ---\n",
    "# Use rglob to recursively find all saved model directories within the main output folder.\n",
    "model_tag = config.MODEL_ID.split('/')[-1]\n",
    "all_model_paths = sorted(list(Path(config.OUTPUT_DIR).rglob(f\"finetuned_{model_tag}\")))\n",
    "\n",
    "if not all_model_paths:\n",
    "    print(\"No trained models found to evaluate. Please ensure the training process completed successfully.\")\n",
    "else:\n",
    "    print(f\"Found {len(all_model_paths)} models to evaluate on the hold-out test set.\")\n",
    "\n",
    "# --- 2. Loop through each model and evaluate it ---\n",
    "for model_to_evaluate_path in all_model_paths:\n",
    "    # Use the parent directory's name for a descriptive title (e.g., L0_Global, L1_Subclass_A)\n",
    "    run_name = model_to_evaluate_path.parent.name\n",
    "    print(f\"\\n{'='*25} EVALUATING: {run_name} {'='*25}\")\n",
    "    print(f\"Model Path: {model_to_evaluate_path}\")\n",
    "\n",
    "    device = get_device()\n",
    "    try:\n",
    "        final_model = AutoModelForSequenceClassification.from_pretrained(model_to_evaluate_path)\n",
    "        final_model.to(device)\n",
    "        # The tokenizer is consistent across all fine-tuning stages\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.MODEL_ID)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load model from {model_to_evaluate_path}. Skipping. Error: {e}\")\n",
    "        continue # Move to the next model\n",
    "\n",
    "    # --- 3. Prepare the Test Dataset ---\n",
    "    # We must use the exact 'label2id' mapping saved with each specific model.\n",
    "    with open(model_to_evaluate_path / 'config.json', 'r') as f:\n",
    "        model_config_data = json.load(f)\n",
    "        label2id = model_config_data['label2id']\n",
    "        id2label = model_config_data['id2label']\n",
    "\n",
    "    # Map the text labels from the test set to the numerical IDs of the current model\n",
    "    # and filter out any test data points whose labels this specific model was not trained on.\n",
    "    final_test_df['numerical_labels'] = final_test_df[config.LABEL_COL].map(label2id)\n",
    "    test_df_filtered = final_test_df.dropna(subset=['numerical_labels'])\n",
    "    \n",
    "    if test_df_filtered.empty:\n",
    "        print(f\"No applicable data found in the hold-out test set for the labels of model '{run_name}'. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    test_labels = test_df_filtered['numerical_labels'].astype(int).tolist()\n",
    "    test_sequences = test_df_filtered[config.SEQ_COL].tolist()\n",
    "    \n",
    "    test_encodings = tokenizer(test_sequences, truncation=True, padding=True)\n",
    "    test_dataset = ProteinDataset(test_encodings, test_labels)\n",
    "\n",
    "    # --- 4. Use the Trainer to Get Predictions ---\n",
    "    final_trainer = Trainer(model=final_model)\n",
    "    predictions, _, _ = final_trainer.predict(test_dataset)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # --- 5. Generate and Save a Unique Report ---\n",
    "    print(f\"\\n--- Final Unbiased Classification Report for {run_name} ---\")\n",
    "    target_names = [id2label[str(i)] for i in sorted(id2label.keys(), key=int)] # Ensure correct label order\n",
    "    final_report_text = classification_report(\n",
    "        test_labels, y_pred, target_names=target_names, zero_division=0\n",
    "    )\n",
    "    print(final_report_text)\n",
    "\n",
    "    # Save the report to a unique file in the main output directory\n",
    "    final_report_path = Path(config.OUTPUT_DIR) / f'FINAL_TEST_REPORT_{run_name}.txt'\n",
    "    with open(final_report_path, 'w') as f:\n",
    "        f.write(f\"--- Final Model Evaluation on the Hold-Out Test Set ---\\n\\n\")\n",
    "        f.write(f\"Model: {run_name}\\n\")\n",
    "        f.write(f\"Model Path: {model_to_evaluate_path}\\n\\n\")\n",
    "        f.write(final_report_text)\n",
    "    print(f\"Final, unbiased report saved to: {final_report_path}\")\n",
    "\n",
    "print(f\"\\n{'='*25} All Evaluations Complete {'='*25}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
