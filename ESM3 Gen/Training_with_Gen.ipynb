{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM Multi-Target Fine-Tuning and Embedding Comparison\n",
    "\n",
    "This notebook walks through the process of using a pre-trained ESM model for a **multi-class classification** task. It generates protein embeddings, fine-tunes the model on multiple targets (including a 'Non-Binder' category), and compares the embedding space before and after fine-tuning using PCA and t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's install and import all the necessary libraries. Ensure you have a GPU available for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers datasets scikit-learn pandas torch seaborn matplotlib accelerate\n",
    "%pip install -q esm biopython py3Dmol httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.special import softmax\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import hashlib\n",
    "import json\n",
    "import configparser\n",
    "\n",
    "# ESM3 Imports for generation\n",
    "from huggingface_hub import login\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfig\n",
    "from esm.sdk import client as cl\n",
    "\n",
    "# Biopython Imports\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "import py3Dmol\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set all your parameters in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = configparser.ConfigParser()\n",
    "credentials.read('../credentials.ini')\n",
    "\n",
    "class Config:\n",
    "    # --- Input and Output ---\n",
    "    # IMPORTANT: Make sure this CSV file is uploaded to your notebook environment!\n",
    "    CSV_PATH = \"../Data/TIMP_binder_MMP3_AB.csv\"\n",
    "    OUTPUT_DIR = \"../esm_gen_out\"\n",
    "    SEQ_COL = \"Full Seq\" # Column with protein sequences\n",
    "    LABEL_COL = \"Target\" # Column with target labels\n",
    "    BINDING_COL = \"Encoding\" # Column indicating positive (1) or negative (0) binding\n",
    "\n",
    "    # --- Model and Training ---\n",
    "    MODEL_ID = \"facebook/esm2_t33_650M_UR50D\"\n",
    "    #MODEL_ID = \"facebook/esm2_t30_150M_UR50D\"\n",
    "    #MODEL_ID = \"facebook/esm2_t6_8M_UR50D\" # Smaller model for testing; switch to larger for better results\n",
    "    #MODEL_ID = \"Synthyra/ESMplusplus_small\" # not functional yet\n",
    "    TEST_SIZE = 0.2\n",
    "    EPOCHS = 25\n",
    "    LEARNING_RATE = 1e-3\n",
    "    DECAY_TYPE = 'linear'\n",
    "    BATCH_SIZE = 8\n",
    "    FORCE_RETRAIN = False # Set to True to retrain even if a saved model exists\n",
    "\n",
    "    # --- Generation Parameters ---\n",
    "    HF_TOKEN = credentials['huggingFace']['token']\n",
    "    FORGE_TOKEN = credentials['forge']['token']\n",
    "    CANONICAL_TEMPLATE_UNIPROT_ID = \"P35625\" # Human TIMP-3\n",
    "    TARGET_UNIPROT_ID = \"P08254\" # Human MMP3\n",
    "    NUM_SEQUENCES_TO_GENERATE = 10000 # Number of new sequences to create for each class\n",
    "    LOOP = \"AB\"\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "These are the utility functions from the script for device detection, embedding computation, and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Detects and returns the available hardware device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available(): # For Apple Silicon\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "def get_uniprot_sequence(accession_id):\n",
    "    \"\"\"Fetches a protein sequence from UniProt.\"\"\"\n",
    "    url = f\"https://www.uniprot.org/uniprot/{accession_id}.fasta\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # The response text is in FASTA format, we need to parse it\n",
    "        fasta_data = response.text.splitlines()\n",
    "        sequence = \"\".join(fasta_data[1:])\n",
    "        return sequence\n",
    "    else:\n",
    "        print(f\"Error fetching sequence for {accession_id}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def compute_embeddings(sequences, model, tokenizer, device):\n",
    "    \"\"\"Computes embeddings for a list of sequences using the given model.\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for seq in tqdm(sequences, desc=\"Computing embeddings\"):\n",
    "            tokens = tokenizer(seq, return_tensors=\"pt\", truncation=True, max_length=1022).to(device)\n",
    "            output = model(**tokens, output_hidden_states=True)\n",
    "            # Use the last hidden state and average pool across the sequence length\n",
    "            embedding = output.hidden_states[-1].mean(dim=1).squeeze().cpu().numpy()\n",
    "            embeddings.append(embedding)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "sns.set_palette([\"#009100\",\"#FF0000\",\"#0000FF\",\"#800080\",\"#FFA500\",\"#00FFFF\",\"#FFC0CB\",\"#A52A2A\",\"#808000\",\"#000000\"])\n",
    "\n",
    "def plot_single(data, labels, title, filename, hue_order=None, style=None):\n",
    "    \"\"\"Creates and saves a single scatter plot.\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.scatterplot(x=data[:, 0], y=data[:, 1], hue=labels, style=style, s=50, alpha=0.7, hue_order=hue_order)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.legend(title=\"Target\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show() # Display the plot inline\n",
    "    plt.close()\n",
    "\n",
    "def plot_comparison(before_data, after_data, labels, reduction_method, out_dir, model_tag, timestamp, hue_order=None):\n",
    "    \"\"\"Creates and saves a 2-panel comparison plot.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    fig.suptitle(f'{reduction_method} Comparison ({model_tag})', fontsize=20)\n",
    "\n",
    "    # Before fine-tuning\n",
    "    sns.scatterplot(ax=axes[0], x=before_data[:, 0], y=before_data[:, 1], hue=labels, s=50, alpha=0.7, hue_order=hue_order)\n",
    "    axes[0].set_title(\"Before Fine-Tuning\", fontsize=16)\n",
    "    axes[0].set_xlabel(\"Dimension 1\")\n",
    "    axes[0].set_ylabel(\"Dimension 2\")\n",
    "    axes[0].legend(title=\"Target\")\n",
    "\n",
    "    # After fine-tuning\n",
    "    sns.scatterplot(ax=axes[1], x=after_data[:, 0], y=after_data[:, 1], hue=labels, s=50, alpha=0.7, hue_order=hue_order)\n",
    "    axes[1].set_title(\"After Fine-Tuning\", fontsize=16)\n",
    "    axes[1].set_xlabel(\"Dimension 1\")\n",
    "    axes[1].set_ylabel(\"Dimension 2\")\n",
    "    axes[1].legend(title=\"Target\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    filename = out_dir / f\"{reduction_method.lower()}_comparison_{model_tag}_{timestamp}.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.show() # Display the plot inline\n",
    "    plt.close()\n",
    "\n",
    "class ProteinDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for protein sequences.\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def compute_metrics_multiclass(pred):\n",
    "    \"\"\"Computes detailed classification metrics for the multi-class Trainer.\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    \n",
    "    # Handle case where pred.predictions is a tuple\n",
    "    preds_data = pred.predictions\n",
    "    if isinstance(preds_data, tuple):\n",
    "        preds_data = preds_data[0]  # take the logits\n",
    "    preds = preds_data.argmax(-1)\n",
    "\n",
    "    # Use 'weighted' average for multi-class classification to account for label imbalance\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Pipeline\n",
    "\n",
    "This is the main execution block. It will perform all the steps from the original script in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths and parameters\n",
    "out_dir = Path(config.OUTPUT_DIR)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "device = get_device()\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_tag = config.MODEL_ID.split('/')[-1]\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using model: {config.MODEL_ID}\")\n",
    "\n",
    "# Load and process data\n",
    "print(\"--- Loading and Processing Data ---\")\n",
    "df = pd.read_csv(config.CSV_PATH)\n",
    "df = df.dropna(subset=[config.SEQ_COL, config.LABEL_COL, config.BINDING_COL]) # Drop rows with missing critical info\n",
    "df = df.drop_duplicates(subset=[config.SEQ_COL]) # Remove duplicate sequences\n",
    "df = df.copy() # Avoid SettingWithCopyWarning\n",
    "\n",
    "# --- Preprocessing Step: Treat non-binders as a separate class ---\n",
    "df.loc[df[config.BINDING_COL] == 0, config.LABEL_COL] = 'Non-Binder'\n",
    "print(f\"Total sequences in dataset: {len(df)}\")\n",
    "\n",
    "# Convert text labels to integers\n",
    "unique_labels = sorted(df[config.LABEL_COL].unique()) # Sort for consistent mapping\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "df['numerical_labels'] = df[config.LABEL_COL].map(label2id)\n",
    "\n",
    "num_labels = len(unique_labels)\n",
    "print(f\"Found {num_labels} unique classes for training: {', '.join(unique_labels)}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df[config.LABEL_COL].value_counts())\n",
    "\n",
    "sequences = df[config.SEQ_COL].tolist()\n",
    "labels = df['numerical_labels'].tolist()\n",
    "text_labels = df[config.LABEL_COL].tolist() # For plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4a: Pre-trained Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 4a: Pre-trained Model Embeddings ---\")\n",
    "\n",
    "# Caching setup for embeddings\n",
    "sequences_hash = hashlib.md5(\"\".join(sequences).encode()).hexdigest()\n",
    "emb_cache_file = out_dir / f\"embeddings_before_{model_tag}_{sequences_hash}.npy\"\n",
    "\n",
    "if emb_cache_file.exists():\n",
    "    print(f\"Loading pre-trained embeddings from cache: {emb_cache_file}\")\n",
    "    embeddings_before = np.load(emb_cache_file)\n",
    "else:\n",
    "    trust_code = 'esm' in config.MODEL_ID.lower()\n",
    "    model = AutoModelForSequenceClassification.from_pretrained( \n",
    "        config.MODEL_ID, \n",
    "        num_labels=num_labels, \n",
    "        trust_remote_code=trust_code\n",
    "    )\n",
    "    if 'facebook/esm2' in config.MODEL_ID.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.MODEL_ID) # facebook/esm2_t33_650M_UR50D uses AutoTokenizer\n",
    "    else:\n",
    "        tokenizer = model.tokenizer  # Use the tokenizer associated with the model\n",
    "\n",
    "    embeddings_before = compute_embeddings(sequences, model, tokenizer, device)\n",
    "    np.save(emb_cache_file, embeddings_before)\n",
    "    print(f\"Saved pre-trained embeddings to cache: {emb_cache_file}\")\n",
    "\n",
    "# PCA and t-SNE on pre-trained embeddings\n",
    "print(\"\\nRunning PCA and t-SNE on pre-trained embeddings...\")\n",
    "pca_before = PCA(n_components=2).fit_transform(embeddings_before)\n",
    "tsne_before = TSNE(n_components=2, perplexity=min(30, len(df)-1), random_state=42).fit_transform(embeddings_before)\n",
    "\n",
    "# ------------------ #\n",
    "# Save results\n",
    "pca_before_csv = out_dir / f'pca_before_{model_tag}_{timestamp}.csv'\n",
    "tsne_before_csv = out_dir / f'tsne_before_{model_tag}_{timestamp}.csv'\n",
    "pd.DataFrame(pca_before, columns=['PC1', 'PC2']).to_csv(pca_before_csv, index=False)\n",
    "pd.DataFrame(tsne_before, columns=['TSNE1', 'TSNE2']).to_csv(tsne_before_csv, index=False)\n",
    "print(\"Saved PCA and t-SNE results for pre-trained embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4b: Fine-tuning Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 4b: Fine-tuning Classification Head ---\")\n",
    "\n",
    "finetuned_model_path = out_dir / f\"finetuned_{model_tag}\"\n",
    "\n",
    "# We need the validation set later for the classification report\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(sequences, labels, test_size=config.TEST_SIZE, random_state=42, stratify=labels)\n",
    "\n",
    "if not finetuned_model_path.exists() or config.FORCE_RETRAIN:\n",
    "    if config.FORCE_RETRAIN and finetuned_model_path.exists():\n",
    "        print(\"Forcing re-training.\")\n",
    "    \n",
    "    trust_code = 'esm' in config.MODEL_ID.lower()\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config.MODEL_ID, \n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        trust_remote_code=trust_code\n",
    "    )\n",
    "\n",
    "    if 'facebook/esm2' in config.MODEL_ID.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.MODEL_ID) # facebook/esm2... uses AutoTokenizer\n",
    "    else:\n",
    "        tokenizer = model.tokenizer  # Use the tokenizer associated with the model\n",
    "\n",
    "\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "    val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "    train_dataset = ProteinDataset(train_encodings, train_labels)\n",
    "    val_dataset = ProteinDataset(val_encodings, val_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(out_dir / 'training_checkpoints'),\n",
    "        num_train_epochs=config.EPOCHS,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=str(out_dir / 'logs'),\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\", # Using f1 for best model selection\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics_multiclass, # Use the multi-class metrics function\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(finetuned_model_path)\n",
    "    tokenizer.save_pretrained(finetuned_model_path)\n",
    "    print(f\"Fine-tuned model saved to {finetuned_model_path}\")\n",
    "\n",
    "    print(\"\\n--- Generating Validation Curve ---\")\n",
    "    # Extract log history\n",
    "    log_history = trainer.state.log_history\n",
    "    \n",
    "    # Use pandas to easily separate training and validation logs\n",
    "    df = pd.DataFrame(log_history)\n",
    "    \n",
    "    # Get training and validation loss data\n",
    "    train_logs = df[df['loss'].notna()].dropna(axis=1, how='all').reset_index(drop=True)\n",
    "    eval_logs = df[df['eval_loss'].notna()].dropna(axis=1, how='all').reset_index(drop=True)\n",
    "\n",
    "    # Plotting\n",
    "    validation_curve_file = out_dir / f\"validation_curve_{model_tag}_{timestamp}.png\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_logs['step'], train_logs['loss'], label='Training Loss')\n",
    "    plt.plot(eval_logs['step'], eval_logs['eval_loss'], label='Validation Loss', marker='o')\n",
    "    \n",
    "    plt.title('Training and Validation Loss Curve')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(validation_curve_file)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "else:\n",
    "    print(f\"Found existing fine-tuned model. Loading from {finetuned_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4c: Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 4c: Detailed Classification Report ---\")\n",
    "\n",
    "# Load the best model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(finetuned_model_path)\n",
    "if 'facebook/esm2' in config.MODEL_ID.lower():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_ID) # facebook/esm2... uses AutoTokenizer\n",
    "else:\n",
    "    tokenizer = model.tokenizer  # Use the tokenizer associated with the model\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Create a dataset for the validation texts\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "val_dataset = ProteinDataset(val_encodings, val_labels)\n",
    "\n",
    "# Re-initialize Trainer to use its `predict` method\n",
    "trainer = Trainer(model=model)\n",
    "predictions, _, _ = trainer.predict(val_dataset)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(f'Classification Report for model: {model_tag}\\n')\n",
    "report_text = classification_report(val_labels, y_pred, target_names=list(label2id.keys()), zero_division=0)\n",
    "print(report_text)\n",
    "\n",
    "# Save report to a file\n",
    "class_report_path = out_dir / f'classification_report_{model_tag}_{timestamp}.txt'\n",
    "with open(class_report_path, 'w') as f:\n",
    "    f.write(report_text)\n",
    "print(f\"\\nClassification report saved to {class_report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4d: Post-fine-tuning Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 4d: Post-fine-tuning Model Embeddings ---\")\n",
    "\n",
    "emb_after_cache_file = out_dir / f\"embeddings_after_{model_tag}_{sequences_hash}.npy\"\n",
    "\n",
    "if emb_after_cache_file.exists() and not config.FORCE_RETRAIN:\n",
    "    print(f\"Loading fine-tuned embeddings from cache: {emb_after_cache_file}\")\n",
    "    embeddings_after = np.load(emb_after_cache_file)\n",
    "else:\n",
    "    # Model and tokenizer are already loaded from the previous step\n",
    "    embeddings_after = compute_embeddings(sequences, model, tokenizer, device)\n",
    "    np.save(emb_after_cache_file, embeddings_after)\n",
    "    print(f\"Saved fine-tuned embeddings to cache: {emb_after_cache_file}\")\n",
    "\n",
    "# PCA and t-SNE on fine-tuned embeddings\n",
    "print(\"\\nRunning PCA and t-SNE on fine-tuned embeddings...\")\n",
    "pca_after = PCA(n_components=2).fit_transform(embeddings_after)\n",
    "tsne_after = TSNE(n_components=2, perplexity=min(30, len(df)-1), random_state=42).fit_transform(embeddings_after)\n",
    "\n",
    "# Save results\n",
    "pca_after_csv = out_dir / f'pca_after_{model_tag}_{timestamp}.csv'\n",
    "tsne_after_csv = out_dir / f'tsne_after_{model_tag}_{timestamp}.csv'\n",
    "pd.DataFrame(pca_after, columns=['PC1', 'PC2']).to_csv(pca_after_csv, index=False)\n",
    "pd.DataFrame(tsne_after, columns=['TSNE1', 'TSNE2']).to_csv(tsne_after_csv, index=False)\n",
    "print(\"Saved PCA and t-SNE results for fine-tuned embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4e: Generating Plots and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 4e: Generating Plots ---\")\n",
    "hue_order = list(label2id.keys())\n",
    "\n",
    "# Define filenames for summary\n",
    "pca_before_png = out_dir / f\"pca_before_{model_tag}_{timestamp}.png\"\n",
    "pca_after_png = out_dir / f\"pca_after_{model_tag}_{timestamp}.png\"\n",
    "tsne_before_png = out_dir / f\"tsne_before_{model_tag}_{timestamp}.png\"\n",
    "tsne_after_png = out_dir / f\"tsne_after_{model_tag}_{timestamp}.png\"\n",
    "pca_comp_png = out_dir / f\"pca_comparison_{model_tag}_{timestamp}.png\"\n",
    "tsne_comp_png = out_dir / f\"tsne_comparison_{model_tag}_{timestamp}.png\"\n",
    "\n",
    "# Individual plots\n",
    "print(\"\\nPCA before fine-tuning:\")\n",
    "plot_single(pca_before, text_labels, \"PCA - Before Fine-tuning\", pca_before_png, hue_order=hue_order)\n",
    "print(\"\\nPCA after fine-tuning:\")\n",
    "plot_single(pca_after, text_labels, \"PCA - After Fine-tuning\", pca_after_png, hue_order=hue_order)\n",
    "print(\"\\nt-SNE before fine-tuning:\")\n",
    "plot_single(tsne_before, text_labels, \"t-SNE - Before Fine-tuning\", tsne_before_png, hue_order=hue_order)\n",
    "print(\"\\nt-SNE after fine-tuning:\")\n",
    "plot_single(tsne_after, text_labels, \"t-SNE - After Fine-tuning\", tsne_after_png, hue_order=hue_order)\n",
    "\n",
    "# Comparison plots\n",
    "print(\"\\nPCA Comparison:\")\n",
    "plot_comparison(pca_before, pca_after, text_labels, \"PCA\", out_dir, model_tag, timestamp, hue_order=hue_order)\n",
    "print(\"\\nt-SNE Comparison:\")\n",
    "plot_comparison(tsne_before, tsne_after, text_labels, \"t-SNE\", out_dir, model_tag, timestamp, hue_order=hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate and Evaluate New Sequences\n",
    "\n",
    "Now, using the fine-tuned model, we will generate new protein sequences based on examples from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5a. Initialize ESM3 Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 5a: Initializing ESM3 Generation Model ---\")\n",
    "login(config.HF_TOKEN)\n",
    "\n",
    "print(\"Setting up ESM3 generation model...\")\n",
    "try:\n",
    "    gen_model: ESM3InferenceClient = ESM3.from_pretrained(\"esm3-open\").to(\"cuda\") # or \"cpu\"\n",
    "    # gen_model: ESM3InferenceClient = cl(\"esm3-medium-2024-08\", token=FORGE_TOKEN)\n",
    "    print(\"ESM3 generation model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load ESM3 model. Error: {e}\")\n",
    "    gen_model = None # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5b: Generate New Sequences for Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_generated_seqs = []\n",
    "\n",
    "# Generate masks for different loops\n",
    "loop = config.LOOP # Options: \"AB\", \"C\", \"EF\", \"GH\", \"Multi\"\n",
    "if loop == \"AB\":\n",
    "    begin_loop = 30\n",
    "    end_loop = 36 # normally 35\n",
    "elif loop == \"C\":\n",
    "    begin_loop = 62\n",
    "    end_loop = 68 # 78\n",
    "elif loop == \"EF\":\n",
    "    begin_loop = 92\n",
    "    end_loop = 96\n",
    "elif loop == \"GH\":\n",
    "    begin_loop = 127\n",
    "    end_loop = 137\n",
    "elif loop == \"Multi\":\n",
    "    begin_loop = 143\n",
    "    end_loop = 153\n",
    "else:\n",
    "    begin_loop = 1\n",
    "    end_loop = 2\n",
    "loop_mask = \"_\" * (end_loop - begin_loop)\n",
    "\n",
    "print(\"\\n--- Step 5b: Generating New Sequences from a Canonical Template ---\")\n",
    "\n",
    "# Get canonical Human TIMP-3 sequence\n",
    "print(f\"Fetching canonical starting sequence from UniProt: {config.CANONICAL_TEMPLATE_UNIPROT_ID}\")\n",
    "template_sequence = get_uniprot_sequence(config.CANONICAL_TEMPLATE_UNIPROT_ID)\n",
    "\n",
    "if template_sequence:\n",
    "    template_sequence = template_sequence[23:] # Remove leading signal peptide for TIMP-3\n",
    "    print(f\"Successfully fetched Human TIMP-3 sequence (UniProt: {config.CANONICAL_TEMPLATE_UNIPROT_ID})\")\n",
    "    print(f\"Sequence Length: {len(template_sequence)}\")\n",
    "    print(f\"Using template: {template_sequence[:30]}...\")\n",
    "    \n",
    "    # Define the loop region to be masked and regenerated\n",
    "    print(f\"Masking loop {loop} from position {begin_loop} to {end_loop} (length {end_loop - begin_loop})\")\n",
    "    masked_prompt = template_sequence[:begin_loop] + loop_mask + template_sequence[end_loop:]\n",
    "    protein_prompt = ESMProtein(sequence=masked_prompt)\n",
    "\n",
    "    for i in tqdm(range(config.NUM_SEQUENCES_TO_GENERATE), desc=\"Generating novel sequences\"):\n",
    "        generated_protein = gen_model.generate(protein_prompt, GenerationConfig(track=\"sequence\", num_steps=8, temperature=1.5)) # default temp is 0.7\n",
    "        all_generated_seqs.append({\"sequence\": generated_protein.sequence})\n",
    "\n",
    "    print(\"\\nProcessing generated sequences to find unique candidates and their frequencies...\")\n",
    "    temp_df = pd.DataFrame(all_generated_seqs)\n",
    "    counts = temp_df['sequence'].value_counts()\n",
    "    generated_df = pd.DataFrame({'sequence': counts.index, 'generation_count': counts.values})\n",
    "    generated_df.to_csv(out_dir / \"generated_sequences.csv\", index=False)\n",
    "    latex_table = generated_df.to_latex(index=False, caption=f\"TIMP-3 {loop} Loop Variants\", label=f\"tab:timp3_loops_{loop}\", escape=False)\n",
    "    with open(out_dir / f\"timp3_{loop}_loops_table.tex\", 'w', encoding='utf-8') as f:\n",
    "        f.write(latex_table)\n",
    "    print(f\"Generated {len(temp_df)} total sequences, resulting in {len(generated_df)} unique sequences.\")\n",
    "else:\n",
    "    print(\"Could not fetch template sequence. Skipping generation.\")\n",
    "    generated_df = pd.DataFrame()\n",
    "\n",
    "print(generated_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise # DONT RUN THIS BLOCK YET\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assume 'config', 'get_unipur_sequence', 'ESMProtein', 'gen_model', \n",
    "# 'GenerationConfig', and 'out_dir' are defined elsewhere in your script.\n",
    "\n",
    "# --- Main List for All Generated Sequences ---\n",
    "# This is now outside the loop to collect sequences from all runs.\n",
    "all_generated_seqs = []\n",
    "\n",
    "# --- Define the loops you want to process ---\n",
    "# This makes it easy to add or remove loops in the future.\n",
    "loops_to_process = [\"AB\", \"C\", \"EF\", \"GH\", \"Multi\"]\n",
    "\n",
    "# --- Fetch the template sequence once ---\n",
    "print(\"\\n--- Step 5b: Fetching Canonical Template ---\")\n",
    "print(f\"Fetching canonical starting sequence from UniProt: {config.CANONICAL_TEMPLATE_UNIPROT_ID}\")\n",
    "template_sequence = get_uniprot_sequence(config.CANONICAL_TEMPLATE_UNIPROT_ID)\n",
    "\n",
    "if template_sequence:\n",
    "    template_sequence = template_sequence[23:] # Remove leading signal peptide for TIMP-3\n",
    "    print(f\"Successfully fetched Human TIMP-3 sequence (UniProt: {config.CANONICAL_TEMPLATE_UNIPROT_ID})\")\n",
    "    print(f\"Sequence Length: {len(template_sequence)}\")\n",
    "    print(f\"Using template: {template_sequence[:30]}...\")\n",
    "\n",
    "    # --- Iterate Over Each Defined Loop ---\n",
    "    for loop in loops_to_process:\n",
    "        print(f\"\\n--- Generating New Sequences for Loop: {loop} ---\")\n",
    "\n",
    "        # Set the mask coordinates for the current loop\n",
    "        if loop == \"AB\":\n",
    "            begin_loop = 30\n",
    "            end_loop = 36\n",
    "        elif loop == \"C\":\n",
    "            begin_loop = 62\n",
    "            end_loop = 68\n",
    "        elif loop == \"EF\":\n",
    "            begin_loop = 92\n",
    "            end_loop = 96\n",
    "        elif loop == \"GH\":\n",
    "            begin_loop = 127\n",
    "            end_loop = 137\n",
    "        elif loop == \"Multi\":\n",
    "            begin_loop = 143\n",
    "            end_loop = 153\n",
    "        else:\n",
    "            # Skip if loop is not defined\n",
    "            print(f\"Warning: Loop '{loop}' is not defined. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        loop_mask = \"_\" * (end_loop - begin_loop)\n",
    "        \n",
    "        # Define the loop region to be masked and regenerated\n",
    "        print(f\"Masking loop {loop} from position {begin_loop} to {end_loop} (length {end_loop - begin_loop})\")\n",
    "        masked_prompt = template_sequence[:begin_loop] + loop_mask + template_sequence[end_loop:]\n",
    "        protein_prompt = ESMProtein(sequence=masked_prompt)\n",
    "\n",
    "        # Generate the specified number of sequences for the current loop\n",
    "        for i in tqdm(range(config.NUM_SEQUENCES_TO_GENERATE), desc=f\"Generating for loop {loop}\"):\n",
    "            generated_protein = gen_model.generate(protein_prompt, GenerationConfig(track=\"sequence\", num_steps=8, temperature=1.5))\n",
    "            # MODIFICATION: Add the current loop name along with the sequence\n",
    "            all_generated_seqs.append({\n",
    "                \"sequence\": generated_protein.sequence,\n",
    "                \"loop\": loop  # This is the new column you wanted\n",
    "            })\n",
    "\n",
    "    # --- Process and Save All Results After the Loop is Done ---\n",
    "    print(\"\\n--- Processing All Generated Sequences ---\")\n",
    "    if all_generated_seqs:\n",
    "        temp_df = pd.DataFrame(all_generated_seqs)\n",
    "        \n",
    "        # MODIFICATION: Group by both sequence and loop to get accurate counts\n",
    "        generated_df = temp_df.groupby(['sequence', 'loop']).size().reset_index(name='generation_count')\n",
    "        \n",
    "        # Optional: Sort for better readability\n",
    "        generated_df = generated_df.sort_values(by=['loop', 'generation_count'], ascending=[True, False])\n",
    "        \n",
    "        # Save the consolidated data to a single CSV file\n",
    "        output_csv_path = out_dir / \"all_generated_sequences.csv\"\n",
    "        generated_df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"Saved all unique sequences to {output_csv_path}\")\n",
    "\n",
    "        # Generate a single LaTeX table for all loops\n",
    "        latex_table = generated_df.to_latex(\n",
    "            index=False,\n",
    "            caption=\"All Generated TIMP-3 Loop Variants\",\n",
    "            label=\"tab:timp3_all_loops\",\n",
    "            escape=False,\n",
    "            longtable=True # Good for long tables\n",
    "        )\n",
    "        output_tex_path = out_dir / \"timp3_all_loops_table.tex\"\n",
    "        with open(output_tex_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(latex_table)\n",
    "        print(f\"Saved LaTeX table to {output_tex_path}\")\n",
    "\n",
    "        print(f\"Generated {len(temp_df)} total sequences, resulting in {len(generated_df)} unique sequence/loop combinations.\")\n",
    "    else:\n",
    "        print(\"No sequences were generated.\")\n",
    "        generated_df = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(\"Could not fetch template sequence. Skipping generation.\")\n",
    "    generated_df = pd.DataFrame()\n",
    "\n",
    "print(\"\\n--- Final DataFrame Head ---\")\n",
    "print(generated_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5c: Analyze Generated Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 5c: Analyzing Generated Sequences with Fine-tuned Model ---\")\n",
    "\n",
    "if generated_df.empty:\n",
    "    print(\"No generated sequences to analyze.\")\n",
    "\n",
    "original_sequences_set = set(df[config.SEQ_COL].tolist())\n",
    "initial_generated_count = len(generated_df)\n",
    "generated_df = generated_df[~generated_df['sequence'].isin(original_sequences_set)].reset_index(drop=True)\n",
    "final_generated_count = len(generated_df)\n",
    "\n",
    "print(f\"Removed {initial_generated_count - final_generated_count} generated sequences that already existed in the original dataset.\")\n",
    "print(f\"Proceeding to analyze {final_generated_count} novel, unique sequences.\")\n",
    "\n",
    "if final_generated_count == 0:\n",
    "    print(\"No novel sequences remained after filtering against the original dataset. Skipping analysis of generated sequences.\")\n",
    "\n",
    "generated_sequences_list = generated_df['sequence'].tolist()\n",
    "\n",
    "# Compute embeddings for the generated sequences\n",
    "print(\"Computing embeddings for generated sequences...\")\n",
    "generated_embeddings = compute_embeddings(generated_sequences_list, model, tokenizer, device)\n",
    "\n",
    "# Predict the class for each generated sequence\n",
    "print(\"Predicting classes for generated sequences...\")\n",
    "gen_encodings = tokenizer(generated_sequences_list, truncation=True, padding=True)\n",
    "# The labels here are just placeholders, they aren't used in prediction\n",
    "gen_dataset = ProteinDataset(gen_encodings, [0] * len(generated_sequences_list))\n",
    "\n",
    "gen_predictions = trainer.predict(gen_dataset)\n",
    "pred_logits = gen_predictions.predictions\n",
    "pred_probs = softmax(pred_logits, axis=1)\n",
    "confidence_scores = np.max(pred_probs, axis=1)\n",
    "predicted_class_ids = np.argmax(pred_logits, axis=1)\n",
    "\n",
    "generated_df['predicted_class'] = [id2label[i] for i in predicted_class_ids]\n",
    "generated_df['confidence'] = confidence_scores\n",
    "\n",
    "generated_df.to_csv(out_dir / \"generated_sequences_confidence.csv\", index=False)\n",
    "\n",
    "# Combine all data and run a single t-SNE for a consistent coordinate space\n",
    "print(\"Running t-SNE on combined experimental and generated data...\")\n",
    "combined_embeddings = np.vstack([embeddings_after, generated_embeddings])\n",
    "tsne_combined = TSNE(n_components=2, perplexity=min(30, len(combined_embeddings)-1), random_state=42).fit_transform(combined_embeddings)\n",
    "\n",
    "# Create a master DataFrame for plotting\n",
    "plot_df = pd.DataFrame()\n",
    "plot_df['label'] = text_labels + generated_df['predicted_class'].tolist()\n",
    "plot_df['source'] = ['Experimental'] * len(text_labels) + ['Generated'] * len(generated_df)\n",
    "plot_df['tsne1'] = tsne_combined[:, 0]\n",
    "plot_df['tsne2'] = tsne_combined[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5d: Genetate Plots From Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the requested plots\n",
    "print(\"Generating new t-SNE plots...\")\n",
    "experimental_df = plot_df[plot_df['source'] == 'Experimental']\n",
    "generated_plot_df = plot_df[plot_df['source'] == 'Generated']\n",
    "\n",
    "# Define filenames for the plots\n",
    "combined_plot_filename = out_dir / f\"tsne_combined_{model_tag}_{timestamp}.png\"\n",
    "experimental_only_filename = out_dir / f\"tsne_experimental_only_{model_tag}_{timestamp}.png\"\n",
    "generated_only_filename = out_dir / f\"tsne_generated_only_{model_tag}_{timestamp}.png\"\n",
    "side_by_side_filename = out_dir / f\"tsne_side_by_side_{model_tag}_{timestamp}.png\"\n",
    "\n",
    "# --- Combined Plot ---\n",
    "print(\"Generating combined t-SNE plot...\")\n",
    "plot_single(plot_df[['tsne1', 'tsne2']].values, plot_df['label'],\n",
    "            \"t-SNE of Experimental and Generated Sequences\", combined_plot_filename,\n",
    "            hue_order=hue_order, style=plot_df['source'])\n",
    "\n",
    "# --- Experimental Data Only ---\n",
    "print(\"Generating experimental data only t-SNE plot...\")\n",
    "plot_single(experimental_df[['tsne1', 'tsne2']].values, experimental_df['label'],\n",
    "            \"t-SNE of Experimental Data Only\", experimental_only_filename,\n",
    "            hue_order=hue_order)\n",
    "\n",
    "# --- Generated Data Only ---\n",
    "print(\"Generating generated data only t-SNE plot...\")\n",
    "plot_single(generated_plot_df[['tsne1', 'tsne2']].values, generated_plot_df['label'],\n",
    "            \"t-SNE of Generated Data Only (Colored by Predicted Class)\", generated_only_filename,\n",
    "            hue_order=hue_order)\n",
    "\n",
    "# --- Side-by-Side Comparison ---\n",
    "print(\"Generating side-by-side t-SNE comparison plot...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "fig.suptitle('t-SNE Comparison: Experimental vs. Generated Data', fontsize=20)\n",
    "sns.scatterplot(ax=axes[0], data=experimental_df, x='tsne1', y='tsne2', hue='label', s=50, alpha=0.7, hue_order=hue_order)\n",
    "axes[0].set_title(\"Experimental Data\", fontsize=16)\n",
    "sns.scatterplot(ax=axes[1], data=generated_plot_df, x='tsne1', y='tsne2', hue='label', s=50, alpha=0.7, hue_order=hue_order)\n",
    "axes[1].set_title(\"Generated Data (Predicted)\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(side_by_side_filename)   \n",
    "plt.show(); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Top 10 Generated Sequences\n",
    "\n",
    "This section identifies and displays the top 10 most confidently predicted sequences for each target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not generated_df.empty:\n",
    "    print(\"\\n--- Top 10 Most Confident Generated Sequences Per Predicted Class ---\")\n",
    "\n",
    "    for class_label in unique_labels:\n",
    "        print(f\"\\n\\n{'='*50}\")\n",
    "        print(f\"Top 10 for class: {class_label}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        top_10 = generated_df[generated_df['predicted_class'] == class_label].sort_values('confidence', ascending=False).head(10)\n",
    "        latex_table = top_10.to_latex(index=False, caption=f\"TIMP-3 {loop} Loop Variants\", label=f\"tab:timp3_loops_{loop}\", escape=False)\n",
    "        with open(out_dir / f\"timp3_{loop}_loops_table_{class_label}_top.tex\", 'w', encoding='utf-8') as f:\n",
    "            f.write(latex_table)\n",
    "\n",
    "        if top_10.empty:\n",
    "            print(\"No generated sequences were confidently predicted for this class.\")\n",
    "        else:\n",
    "            for i, row in top_10.iterrows():\n",
    "                print(f\"  Confidence: {row.confidence:.4f} | Sequence: {row.sequence}\")\n",
    "else:\n",
    "    print(\"\\n--- No generated sequences to analyze ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validate Top Candidates\n",
    "\n",
    "This section performs computational validation on the top-ranked sequences. We will check for structural integrity (pLDDT), prepare files for binding prediction, and assess sequence plausibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's collect the top unique sequences from the previous step into a dictionary.\n",
    "top_candidates = {}\n",
    "if not generated_df.empty and 'predicted_class' in generated_df.columns:\n",
    "    for class_label in unique_labels:\n",
    "        #if class_label == 'Non-Binder': continue\n",
    "        \n",
    "        top_sequences_for_class = generated_df[\n",
    "            generated_df['predicted_class'] == class_label\n",
    "        ].sort_values(\n",
    "            by=['confidence', 'generation_count'], ascending=[False, False]\n",
    "        ).head(10)\n",
    "        \n",
    "        if not top_sequences_for_class.empty:\n",
    "            top_candidates[class_label] = top_sequences_for_class['sequence'].tolist()\n",
    "\n",
    "if not top_candidates:\n",
    "    print(\"No top candidates were identified in the previous step. Skipping validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat map\n",
    "\n",
    "Create heat maps of the amino acid generation frequency for all, then for each class. Then generate grouped heat maps for various biochemical properties of amino acids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- New Section: Amino Acid Group Definitions ---\n",
    "# You can customize these groups or add new ones as needed.\n",
    "GROUPINGS = {\n",
    "    'charge': {\n",
    "        'Positive': ['R', 'H', 'K'],\n",
    "        'Negative': ['D', 'E'],\n",
    "        'Neutral': ['A', 'N', 'C', 'Q', 'G', 'I', 'L', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "    },\n",
    "    'hydropathy': {\n",
    "        'Hydrophobic': ['A', 'V', 'I', 'L', 'M', 'F', 'Y', 'W'],\n",
    "        'Hydrophilic': ['R', 'N', 'D', 'C', 'Q', 'E', 'H', 'K', 'S', 'T'],\n",
    "        'Neutral': ['G', 'P']\n",
    "    },\n",
    "    'size': {\n",
    "        'Tiny': ['A', 'C', 'G', 'S'],\n",
    "        'Small': ['D', 'N', 'P', 'T', 'V'],\n",
    "        'Medium': ['E', 'H', 'I', 'L', 'K', 'M', 'Q'],\n",
    "        'Large': ['F', 'R', 'W', 'Y']\n",
    "    },\n",
    "    'chemical': {\n",
    "        'Aliphatic': ['A', 'G', 'I', 'L', 'P', 'V'],\n",
    "        'Aromatic': ['F', 'W', 'Y'],\n",
    "        'Acidic': ['D', 'E'],\n",
    "        'Basic': ['R', 'H', 'K'],\n",
    "        'Hydroxylic': ['S', 'T'],\n",
    "        'Amide': ['N', 'Q'],\n",
    "        'Sulfur': ['C', 'M']\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- New Section: Helper Function for Grouped Heatmaps ---\n",
    "def generate_grouped_heatmap(freq_df, group_dict, title, filename):\n",
    "    \"\"\"\n",
    "    Generates and saves a heatmap based on grouped amino acid frequencies.\n",
    "\n",
    "    Args:\n",
    "        freq_df (pd.DataFrame): DataFrame with amino acids as index and positions as columns.\n",
    "        group_dict (dict): Dictionary defining the amino acid groups.\n",
    "        title (str): Title for the plot.\n",
    "        filename (str or Path): Path to save the output image.\n",
    "    \"\"\"\n",
    "    # Create a mapping from each amino acid to its group\n",
    "    aa_to_group_map = {aa: group for group, aa_list in group_dict.items() for aa in aa_list}\n",
    "    \n",
    "    # Use the map to group the DataFrame and sum the frequencies\n",
    "    grouped_freq_df = freq_df.groupby(aa_to_group_map).sum()\n",
    "    \n",
    "    # Ensure the order of groups in the heatmap is consistent\n",
    "    group_order = list(group_dict.keys())\n",
    "    grouped_freq_df = grouped_freq_df.reindex(group_order)\n",
    "\n",
    "    # Generate and save the heatmap\n",
    "    plt.figure(figsize=(10, max(4, len(group_order) * 0.8)))\n",
    "    sns.heatmap(grouped_freq_df, cmap='viridis', annot=True, fmt=\".2f\", linewidths=.5)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Position in Sequence', fontsize=12)\n",
    "    plt.ylabel('Amino Acid Group', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"  Saved grouped heatmap: '{filename}'\")\n",
    "\n",
    "generated_df = pd.read_csv(out_dir / \"generated_sequences_confidence.csv\") # Reread from file\n",
    "begin_loop = 30; end_loop = 36\n",
    "\n",
    "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "loop_positions = list(range(begin_loop, end_loop))\n",
    "\n",
    "all_candidates_by_class = {}\n",
    "if 'predicted_class' in generated_df.columns:\n",
    "    all_candidates_by_class = generated_df.groupby('predicted_class')['sequence'].apply(list).to_dict()\n",
    "\n",
    "if not all_candidates_by_class:\n",
    "    print(\"No sequences were found or the 'predicted_class' column is missing.\")\n",
    "else:\n",
    "    # === Generate a single heatmap for ALL sequences combined ===\n",
    "    print(\"Generating combined heatmap for all sequences...\")\n",
    "    all_sequences = [seq for sublist in all_candidates_by_class.values() for seq in sublist]\n",
    "    loop_sequences = [seq[begin_loop:end_loop] for seq in all_sequences]\n",
    "\n",
    "    freq_df_all = pd.DataFrame(0, index=list(amino_acids), columns=loop_positions)\n",
    "    for loop in loop_sequences:\n",
    "        if len(loop) == (end_loop - begin_loop):\n",
    "            for i, aa in enumerate(loop):\n",
    "                position = begin_loop + i\n",
    "                if aa in freq_df_all.index:\n",
    "                    freq_df_all.loc[aa, position] += 1\n",
    "\n",
    "    if all_sequences:\n",
    "        freq_df_all = freq_df_all / len(all_sequences)\n",
    "\n",
    "    freq_df_all.columns = [pos + 1 for pos in freq_df_all.columns]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.heatmap(freq_df_all, cmap='viridis', annot=False) # Annot set to False for clarity on dense plots\n",
    "    plt.title('Amino Acid Frequency in Loop (All Candidates)', fontsize=16)\n",
    "    plt.xlabel('Position in Sequence', fontsize=12)\n",
    "    plt.ylabel('Amino Acid', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    all_filename = out_dir / f'heatmap_all_candidates_individual_{config.LOOP}_{timestamp}.png'\n",
    "    plt.savefig(all_filename)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"Saved '{all_filename}'\")\n",
    "\n",
    "    # --- New: Generate Grouped Heatmaps for ALL sequences ---\n",
    "    print(\"\\nGenerating grouped heatmaps for all sequences...\")\n",
    "    for group_name, group_dict in GROUPINGS.items():\n",
    "        title = f'Grouped AA Frequency ({group_name.capitalize()}) - All Candidates'\n",
    "        filename = out_dir / f'heatmap_all_candidates_grouped_by_{group_name}_{config.LOOP}_{timestamp}.png'\n",
    "        generate_grouped_heatmap(freq_df_all, group_dict, title, filename)\n",
    "\n",
    "    # === Generate a separate heatmap for EACH class ===\n",
    "    print(\"\\nGenerating separate heatmaps for each class...\")\n",
    "    for class_label, sequences in all_candidates_by_class.items():\n",
    "        print(f\"- Processing class: {class_label}\")\n",
    "        loop_sequences_class = [seq[begin_loop:end_loop] for seq in sequences]\n",
    "\n",
    "        freq_df_class = pd.DataFrame(0, index=list(amino_acids), columns=loop_positions)\n",
    "        for loop in loop_sequences_class:\n",
    "            if len(loop) == (end_loop - begin_loop):\n",
    "                for i, aa in enumerate(loop):\n",
    "                    position = begin_loop + i\n",
    "                    if aa in freq_df_class.index:\n",
    "                        freq_df_class.loc[aa, position] += 1\n",
    "        \n",
    "        if sequences:\n",
    "            freq_df_class = freq_df_class / len(sequences)\n",
    "\n",
    "        freq_df_class.columns = [pos + 1 for pos in freq_df_class.columns]\n",
    "\n",
    "        plt.figure(figsize=(8, 10))\n",
    "        sns.heatmap(freq_df_class, cmap='viridis', annot=False) # Annot set to False\n",
    "        plt.title(f'Amino Acid Frequency for Class: {class_label}', fontsize=16)\n",
    "        plt.xlabel('Position in Sequence', fontsize=12)\n",
    "        plt.ylabel('Amino Acid', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        class_filename = out_dir / f'heatmap_for_{class_label}_individual_{timestamp}.png'\n",
    "        plt.savefig(class_filename)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        print(f\"  Saved '{class_filename}'\")\n",
    "\n",
    "        # --- New: Generate Grouped Heatmaps for EACH class ---\n",
    "        print(f\"  Generating grouped heatmaps for class: {class_label}...\")\n",
    "        for group_name, group_dict in GROUPINGS.items():\n",
    "            title = f'Grouped AA Frequency ({group_name.capitalize()}) - Class: {class_label}'\n",
    "            filename = out_dir / f'heatmap_for_{class_label}_grouped_by_{group_name}_{config.LOOP}_{timestamp}.png'\n",
    "            generate_grouped_heatmap(freq_df_class, group_dict, title, filename)\n",
    "\n",
    "    print(\"\\nAll heatmaps generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7a: Predict 3D Structure and Assess Integrity (pLDDT)\n",
    "A good candidate should fold into a high-confidence 3D structure. ESM3 can predict this structure and provide a pLDDT score (predicted Local Distance Difference Test) for each amino acid, where >90 is very high confidence and >70 is generally confident. We'll visualize the top candidate for the first class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Predicting 3D structures for top candidates ---\")\n",
    "validation_dir = out_dir / \"validation_outputs\"\n",
    "validation_dir.mkdir(exist_ok=True)\n",
    "pdb_files = {}\n",
    "\n",
    "if gen_model:\n",
    "    for class_label, sequences_list in top_candidates.items():\n",
    "        print(f\"\\nProcessing top candidates for: {class_label}\")\n",
    "        pdb_files[class_label] = []\n",
    "        for i, seq in enumerate(sequences_list):\n",
    "            try:\n",
    "                protein = ESMProtein(sequence=seq)\n",
    "                # Use the generation model to predict structure\n",
    "                structure = gen_model.generate(protein, GenerationConfig(track=\"structure\", num_steps=8))\n",
    "                \n",
    "                pdb_filename = validation_dir / f\"{class_label}_candidate_{i+1}.pdb\"\n",
    "                structure.to_pdb(str(pdb_filename))\n",
    "                pdb_files[class_label].append(str(pdb_filename))\n",
    "                print(f\"Saved structure for {class_label} candidate {i+1} to {pdb_filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not predict structure for {class_label} candidate {i+1}. Error: {e}\")\n",
    "    \n",
    "    first_class = list(top_candidates.keys())[0]\n",
    "    first_pdb_file = pdb_files.get(first_class, [None])[0]\n",
    "\n",
    "    if first_pdb_file:\n",
    "        print(f\"\\n--- Visualizing structure for the #1 candidate of the '{first_class}' class ---\")\n",
    "        with open(first_pdb_file, 'r') as f:\n",
    "            pdb_data = f.read()\n",
    "\n",
    "        view = py3Dmol.view(width=800, height=600)\n",
    "        view.addModel(pdb_data, \"pdb\")\n",
    "        view.setStyle({'cartoon': {'colorscheme': 'pLDDT'}})\n",
    "        view.zoomTo()\n",
    "        view.show()\n",
    "else:\n",
    "    print(\"ESM3 generation model not loaded. Skipping structure prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7b: Prepare for Binding Prediction (Co-folding)\n",
    "To predict if the variant binds to MMP-9, we can use a co-folding tool like AlphaFold-Multimer or ESMFold-Multimer. This requires a FASTA file containing both the target (MMP-9) and the candidate sequence. This step prepares those files.\n",
    "\n",
    "The FASTA files are then uploaded to Google drive where ColabFold will check the folding and binding properties (https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/batch/AlphaFold2_batch.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Preparing FASTA files for co-folding analysis ---\")\n",
    "fasta_dir = validation_dir / \"cofold_fasta_files\"\n",
    "fasta_dir.mkdir(exist_ok=True)\n",
    "\n",
    "uniprot_mmp3 = \"P08254\"\n",
    "uniprot_mmp9 = \"P14780\"\n",
    "uniprot_adam17 = \"P78536\"\n",
    "for class_label, sequences_list in top_candidates.items():\n",
    "    if class_label == \"MMP3\":\n",
    "        target_sequence = get_uniprot_sequence(uniprot_mmp3)\n",
    "    elif class_label == \"MMP9\":\n",
    "        target_sequence = get_uniprot_sequence(uniprot_mmp9)\n",
    "    elif class_label == \"ADAM17\":\n",
    "        target_sequence = get_uniprot_sequence(uniprot_adam17)\n",
    "    else:\n",
    "        target_sequence = get_uniprot_sequence(config.TARGET_UNIPROT_ID)\n",
    "    #print(f\"Fetching target sequence from UniProt: {config.TARGET_UNIPROT_ID}\")\n",
    "    print(f\"Sequence Length: {len(target_sequence)}aa\")\n",
    "    print(f\"Sequence: {target_sequence[:60]}...\") # Uncomment to view\n",
    "    for i, seq in enumerate(sequences_list):\n",
    "        fasta_filename = fasta_dir / f\"{class_label}_candidate_{i+1}_vs_TIMP3.fasta\"\n",
    "        with open(fasta_filename, 'w') as f:\n",
    "            f.write(f\">{class_label} Target Protein\\n{target_sequence}\\n\")\n",
    "            f.write(f\">{class_label}_Candidate_{i+1} Predicted Binder\\n{seq}\\n\")\n",
    "\n",
    "        complex_fasta_filename = fasta_dir / f\"complex_{class_label}_candidate_{i+1}.fasta\"\n",
    "        with open(complex_fasta_filename, 'w') as f:\n",
    "            f.write(f\">{class_label}_complex_candidate_{i+1}\\n\")\n",
    "            # Write both sequences on the same line, separated by a colon\n",
    "            f.write(f\"{target_sequence}:{seq}\\n\")\n",
    "print(f\"\\nFASTA files for all top candidates are saved in '{fasta_dir}'.\")\n",
    "print(\"These files can now be submitted to a co-folding service like AlphaFold-Multimer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7c: Analyze AlphaFold Output from File\n",
    "\n",
    "Automated Batch Validation of ColabFold Outputs. This cell automates the analysis of all ColabFold `.zip` outputs.\n",
    "**Action:**\n",
    "1.  Run your candidate FASTA files through the ColabFold notebook.\n",
    "2.  Download the resulting `.zip` files.\n",
    "3.  Place all of them inside the `validation_outputs` directory created by the previous step.\n",
    "4.  Run this cell. It will generate a full report for every candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# --- SETUP: Define the directory where you placed your zip files ---\n",
    "out_dir = Path(config.OUTPUT_DIR)\n",
    "validation_dir = out_dir / \"validation_outputs\"\n",
    "if not validation_dir.exists():\n",
    "    print(f\"Creating directory: {validation_dir}\")\n",
    "    validation_dir.mkdir()\n",
    "\n",
    "print(f\"Searching for ColabFold .zip outputs in: {validation_dir}\")\n",
    "\n",
    "# Find all zip files in the directory\n",
    "zip_files = list(validation_dir.glob(\"*.zip\"))\n",
    "\n",
    "candidate_num = \"001\"\n",
    "\n",
    "if not zip_files:\n",
    "    print(\"\\nERROR: No .zip files found in the validation directory.\")\n",
    "    print(\"Please make sure to place your ColabFold outputs there before running this cell.\")\n",
    "else:\n",
    "    print(f\"Found {len(zip_files)} candidate zip files to analyze.\")\n",
    "    \n",
    "    results_list = []\n",
    "\n",
    "    # --- LOOP THROUGH EACH ZIP FILE AND ANALYZE ---\n",
    "    for zip_file_path in sorted(zip_files):\n",
    "        candidate_name = zip_file_path.stem\n",
    "        display(HTML(f\"<hr><h2>Analyzing Candidate: {candidate_name}</h2>\"))\n",
    "        \n",
    "        # Unzip the contents into a subdirectory\n",
    "        unzip_dir = validation_dir / candidate_name\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(unzip_dir)\n",
    "        \n",
    "        # Find the files for the top-ranked model (rank 1)\n",
    "        try:\n",
    "            scores_file = next(unzip_dir.glob(f\"*_scores_rank_{candidate_num}_*.json\"))\n",
    "            pae_file = next(unzip_dir.glob(\"*_predicted_aligned_error_v1.json\"))\n",
    "            pdb_file = next(unzip_dir.glob(f\"*_unrelaxed_rank_{candidate_num}_*.pdb\"))\n",
    "        except StopIteration:\n",
    "            print(f\"WARNING: Could not find all required output files for rank 1 in {candidate_name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # --- EXTRACT AND DISPLAY KEY SCORES ---\n",
    "        with open(scores_file, 'r') as f:\n",
    "            scores_data = json.load(f)\n",
    "        \n",
    "        ipTM_score = scores_data.get(\"iptm\")\n",
    "        mean_plddt = np.mean(scores_data.get(\"plddt\", [0]))\n",
    "        \n",
    "        results_list.append({\n",
    "            \"Candidate\": candidate_name,\n",
    "            \"ipTM\": ipTM_score,\n",
    "            \"Mean pLDDT\": mean_plddt\n",
    "        })\n",
    "\n",
    "        print(\"\\n--- Confidence Scores ---\")\n",
    "        if ipTM_score is not None:\n",
    "            print(f\"Interface pTM (ipTM): {ipTM_score:.4f}\")\n",
    "            if ipTM_score > 0.85:\n",
    "                print(\"   Interpretation: High confidence binding prediction.\")\n",
    "            else:\n",
    "                print(\"   Interpretation: Low confidence binding prediction.\")\n",
    "        \n",
    "        if mean_plddt > 0:\n",
    "             print(f\"Mean pLDDT: {mean_plddt:.2f} (Overall structure confidence)\")\n",
    "        \n",
    "        # --- PLOT THE PAE MATRIX FROM RAW JSON DATA ---\n",
    "        with open(pae_file, 'r') as f:\n",
    "            pae_data = json.load(f)\n",
    "        \n",
    "        pae_matrix = pae_data.get('predicted_aligned_error')\n",
    "        if pae_matrix:\n",
    "            print(\"\\n--- Predicted Aligned Error (PAE) Plot ---\")\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(pae_matrix, cmap='Greens_r', vmin=0, vmax=30)\n",
    "            plt.colorbar(label=\"Expected Position Error ()\")\n",
    "            plt.title(f\"PAE Plot for {candidate_name}\")\n",
    "            plt.xlabel(\"Scored Residue\")\n",
    "            plt.ylabel(\"Aligned Residue\")\n",
    "            plt.savefig(validation_dir / f\"PAE_{candidate_name}_{candidate_num}.png\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "        # --- VISUALIZE THE 3D STRUCTURE ---\n",
    "        print(\"\\n--- Predicted 3D Structure (Colored by pLDDT) ---\")\n",
    "        with open(pdb_file, 'r') as f:\n",
    "            pdb_data = f.read()\n",
    "\n",
    "        view = py3Dmol.view(width=800, height=600)\n",
    "        view.addModel(pdb_data, \"pdb\")\n",
    "        view.setStyle({'cartoon': {'colorscheme': 'pLDDT'}})\n",
    "        view.zoomTo()\n",
    "        view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FINAL SUMMARY ---\n",
    "if results_list:\n",
    "    display(HTML(\"<hr><h1>Final Summary Ranking</h1>\"))\n",
    "    summary_df = pd.DataFrame(results_list)\n",
    "    #summary_df['Cand Enc'] = pd.factorize(summary_df['Candidate'].str[:-10])[0]\n",
    "    summary_df_sorted = summary_df.sort_values(by=\"ipTM\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Style the dataframe for better readability\n",
    "    styled_summary_df = summary_df_sorted.style.background_gradient(subset=['ipTM'], cmap='viridis').format({'ipTM': '{:.4f}', 'Mean pLDDT': '{:.2f}'})\n",
    "    styled_summary_df = styled_summary_df.background_gradient(subset=['Mean pLDDT'], cmap='viridis')\n",
    "    #styled_summary_df = styled_summary_df.background_gradient(subset=['Cand Enc'], cmap='viridis')\n",
    "\n",
    "    display(styled_summary_df)\n",
    "    \n",
    "    styled_summary_df.to_excel(validation_dir / 'AlphaFold_Summary.xlsx', engine='openpyxl', index=False)\n",
    "    latex_table = styled_summary_df.hide(axis=\"index\").to_latex(caption=f\"AlphaFold2-multimer Summary\", label=f\"tab:af2_multi_sum\", convert_css=True)\n",
    "    latex_table = latex_table.replace(\"_\", \" \").replace(\".result\", \"\")\n",
    "    with open(validation_dir / f\"AlphaFold_Summary.tex\", 'w', encoding='utf-8') as f:\n",
    "        f.write(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[DONE] All tasks completed!\")\n",
    "print(\"\\n--- Summary of Output Files ---\")\n",
    "print(f\"- Fine-tuned model saved to: {out_dir}\")\n",
    "print(f\"- Classification report saved to: {class_report_path}\")\n",
    "print(\"\\n- Embeddings:\")\n",
    "print(f\"  - Before fine-tuning (cached): {emb_cache_file}\")\n",
    "print(f\"  - After fine-tuning (cached): {emb_after_cache_file}\")\n",
    "print(\"\\n- CSV Coordinates:\")\n",
    "print(f\"  - PCA (before): {pca_before_csv}\")\n",
    "print(f\"  - t-SNE (before): {tsne_before_csv}\")\n",
    "print(f\"  - PCA (after): {pca_after_csv}\")\n",
    "print(f\"  - t-SNE (after): {tsne_after_csv}\")\n",
    "print(\"\\n- Plots:\")\n",
    "print(f\"  - PCA Comparison: {pca_comp_png}\")\n",
    "print(f\"  - t-SNE Comparison: {tsne_comp_png}\")\n",
    "print(f\"  - Validation Curve: {validation_curve_file}\")\n",
    "print(f\"  - Combined t-SNE Plot: {combined_plot_filename}\")\n",
    "print(f\"  - Experimental Only t-SNE Plot: {experimental_only_filename}\")\n",
    "print(f\"  - Generated Only t-SNE Plot: {generated_only_filename}\")\n",
    "print(f\"  - Side-by-Side t-SNE Comparison: {side_by_side_filename}\")\n",
    "print(\"\\nPlease check the output directory for all generated files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
